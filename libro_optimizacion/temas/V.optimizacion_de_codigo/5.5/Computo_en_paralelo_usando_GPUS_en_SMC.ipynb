{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(COMPPARALELOGPUSSMC)="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5 Cómputo en paralelo usando GPUs en un sistema de memoria compartida (SMC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Notas para contenedor de docker:\n",
    "\n",
    "Comando de docker para ejecución de la nota de forma local:\n",
    "\n",
    "nota: cambiar `<ruta a mi directorio>` por la ruta de directorio que se desea mapear a `/datos` dentro del contenedor de docker.\n",
    "\n",
    "`docker run --rm -v <ruta a mi directorio>:/datos --name jupyterlab_optimizacion_2 -p 8888:8888 -p 8787:8787 -d palmoreck/jupyterlab_optimizacion_2:3.0.0`\n",
    "\n",
    "password para jupyterlab: `qwerty`\n",
    "\n",
    "Detener el contenedor de docker:\n",
    "\n",
    "`docker stop jupyterlab_optimizacion_2`\n",
    "\n",
    "Documentación de la imagen de docker `palmoreck/jupyterlab_optimizacion_2:3.0.0` en [liga](https://github.com/palmoreck/dockerfiles/tree/master/jupyterlab/optimizacion_2).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota generada a partir de [liga](https://www.dropbox.com/s/yjijtfuky3s5dfz/2.5.Compute_Unified_Device_Architecture.pdf?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Al final de esta nota el y la lectora:\n",
    ":class: tip\n",
    "\n",
    "*\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se presentan códigos y sus ejecuciones en una máquina `<falta colocar>` de la nube de [AWS](https://aws.amazon.com/). Se utilizó la AMI `opt2-aws-educate-openblas-04-04-2021` de la región `us-east-1` (Virginia) para reproducibilidad de resultados. Tal AMI se construyó a partir de una AMI `ubuntu 20.04 - ami-042e8287309f5df03` con el [script_profiling_and_BLAS.sh](https://github.com/palmoreck/scripts_for_useful_tools_installations/blob/main/AWS/ubuntu_20.04/optimizacion_2/script_profiling_and_BLAS.sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{admonition} Comentario\n",
    "\n",
    "Si se utiliza la *AMI* `opt2-aws-educate-openblas-04-04-2021` colocar en `User data` el siguiente *script*:\n",
    "\n",
    "```bash\n",
    "\n",
    "#!/bin/bash\n",
    "##variables:\n",
    "region=us-east-1 #make sure instance is in Virginia\n",
    "name_instance=OpenBLAS\n",
    "USER=ubuntu\n",
    "##System update\n",
    "apt-get update -yq\n",
    "##Tag instance\n",
    "INSTANCE_ID=$(curl -s http://instance-data/latest/meta-data/instance-id)\n",
    "PUBLIC_IP=$(curl -s http://instance-data/latest/meta-data/public-ipv4)\n",
    "sudo -H -u $USER bash -c \"/home/$USER/.local/bin/aws ec2 create-tags --resources $INSTANCE_ID --tag Key=Name,Value=$name_instance-$PUBLIC_IP --region=$region\"\n",
    "sudo -H -u $USER bash -c \"cd / && /home/$USER/.local/bin/jupyter lab --ip=0.0.0.0 --no-browser --config=/home/$USER/.jupyter/jupyter_notebook_config.py &\"\n",
    "\n",
    "```\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La máquina `<falta colocar>` tiene las siguientes características:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Falta ejecutar de acuerdo a la máquina elegida**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:                    x86_64\n",
      "CPU op-mode(s):                  32-bit, 64-bit\n",
      "Byte Order:                      Little Endian\n",
      "Address sizes:                   46 bits physical, 48 bits virtual\n",
      "CPU(s):                          64\n",
      "On-line CPU(s) list:             0-63\n",
      "Thread(s) per core:              2\n",
      "Core(s) per socket:              16\n",
      "Socket(s):                       2\n",
      "NUMA node(s):                    2\n",
      "Vendor ID:                       GenuineIntel\n",
      "CPU family:                      6\n",
      "Model:                           79\n",
      "Model name:                      Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz\n",
      "Stepping:                        1\n",
      "CPU MHz:                         2290.913\n",
      "CPU max MHz:                     3000.0000\n",
      "CPU min MHz:                     1200.0000\n",
      "BogoMIPS:                        4600.03\n",
      "Hypervisor vendor:               Xen\n",
      "Virtualization type:             full\n",
      "L1d cache:                       1 MiB\n",
      "L1i cache:                       1 MiB\n",
      "L2 cache:                        8 MiB\n",
      "L3 cache:                        90 MiB\n",
      "NUMA node0 CPU(s):               0-15,32-47\n",
      "NUMA node1 CPU(s):               16-31,48-63\n",
      "Vulnerability Itlb multihit:     KVM: Vulnerable\n",
      "Vulnerability L1tf:              Mitigation; PTE Inversion\n",
      "Vulnerability Mds:               Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\n",
      "Vulnerability Meltdown:          Mitigation; PTI\n",
      "Vulnerability Spec store bypass: Vulnerable\n",
      "Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
      "Vulnerability Spectre v2:        Mitigation; Full generic retpoline, STIBP disabled, RSB filling\n",
      "Vulnerability Srbds:             Not affected\n",
      "Vulnerability Tsx async abort:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\n",
      "Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq monitor est ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm cpuid_fault invpcid_single pti fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm xsaveopt ida\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  *-firmware\n",
      "       description: BIOS\n",
      "       vendor: Xen\n",
      "       physical id: 0\n",
      "       version: 4.11.amazon\n",
      "       date: 08/24/2006\n",
      "       size: 96KiB\n",
      "       capabilities: pci edd\n",
      "  *-memory\n",
      "       description: System Memory\n",
      "       physical id: 1000\n",
      "       size: 256GiB\n",
      "       capabilities: ecc\n",
      "       configuration: errordetection=multi-bit-ecc\n",
      "     *-bank:0\n",
      "          description: DIMM RAM\n",
      "          physical id: 0\n",
      "          slot: DIMM 0\n",
      "          size: 16GiB\n",
      "          width: 64 bits\n",
      "     *-bank:1\n",
      "          description: DIMM RAM\n",
      "          physical id: 1\n",
      "          slot: DIMM 1\n",
      "          size: 16GiB\n",
      "          width: 64 bits\n",
      "     *-bank:2\n",
      "          description: DIMM RAM\n",
      "          physical id: 2\n",
      "          slot: DIMM 2\n",
      "          size: 16GiB\n",
      "          width: 64 bits\n",
      "     *-bank:3\n",
      "          description: DIMM RAM\n",
      "          physical id: 3\n",
      "          slot: DIMM 3\n",
      "          size: 16GiB\n",
      "          width: 64 bits\n",
      "     *-bank:4\n",
      "          description: DIMM RAM\n",
      "          physical id: 4\n",
      "          slot: DIMM 4\n",
      "          size: 16GiB\n",
      "          width: 64 bits\n",
      "     *-bank:5\n",
      "          description: DIMM RAM\n",
      "          physical id: 5\n",
      "          slot: DIMM 5\n",
      "          size: 16GiB\n",
      "          width: 64 bits\n",
      "     *-bank:6\n",
      "          description: DIMM RAM\n",
      "          physical id: 6\n",
      "          slot: DIMM 6\n",
      "          size: 16GiB\n",
      "          width: 64 bits\n",
      "     *-bank:7\n",
      "          description: DIMM RAM\n",
      "          physical id: 7\n",
      "          slot: DIMM 7\n",
      "          size: 16GiB\n",
      "          width: 64 bits\n",
      "     *-bank:8\n",
      "          description: DIMM RAM\n",
      "          physical id: 8\n",
      "          slot: DIMM 8\n",
      "          size: 16GiB\n",
      "          width: 64 bits\n",
      "     *-bank:9\n",
      "          description: DIMM RAM\n",
      "          physical id: 9\n",
      "          slot: DIMM 9\n",
      "          size: 16GiB\n",
      "          width: 64 bits\n",
      "     *-bank:10\n",
      "          description: DIMM RAM\n",
      "          physical id: a\n",
      "          slot: DIMM 10\n",
      "          size: 16GiB\n",
      "          width: 64 bits\n",
      "     *-bank:11\n",
      "          description: DIMM RAM\n",
      "          physical id: b\n",
      "          slot: DIMM 11\n",
      "          size: 16GiB\n",
      "          width: 64 bits\n",
      "     *-bank:12\n",
      "          description: DIMM RAM\n",
      "          physical id: c\n",
      "          slot: DIMM 12\n",
      "          size: 16GiB\n",
      "          width: 64 bits\n",
      "     *-bank:13\n",
      "          description: DIMM RAM\n",
      "          physical id: d\n",
      "          slot: DIMM 13\n",
      "          size: 16GiB\n",
      "          width: 64 bits\n",
      "     *-bank:14\n",
      "          description: DIMM RAM\n",
      "          physical id: e\n",
      "          slot: DIMM 14\n",
      "          size: 16GiB\n",
      "          width: 64 bits\n",
      "     *-bank:15\n",
      "          description: DIMM RAM\n",
      "          physical id: f\n",
      "          slot: DIMM 15\n",
      "          size: 16GiB\n",
      "          width: 64 bits\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sudo lshw -C memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux ip-10-0-0-140 5.4.0-1038-aws #40-Ubuntu SMP Fri Feb 5 23:50:40 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "uname -ar #r for kernel, a for all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Observación\n",
    ":class: tip\n",
    "\n",
    "En la celda anterior se utilizó el comando de *magic* `%%bash`. Algunos comandos de *magic* los podemos utilizar también con `import`. Ver [ipython-magics](https://ipython.readthedocs.io/en/stable/interactive/magics.html#)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [CUDA-C](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [CuPy](https://github.com/cupy/cupy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Gputools](https://github.com/nullsatz/gputools) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver [gputools: cran](https://rdrr.io/cran/gputools/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias de interés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para más sobre *Unified Memory* revisar:\n",
    "\n",
    "* [Even easier introduction to cuda](https://devblogs.nvidia.com/even-easier-introduction-cuda/)\n",
    "\n",
    "* [Unified memory cuda beginners](https://devblogs.nvidia.com/unified-memory-cuda-beginners/)\n",
    "\n",
    "Es importante el manejo de errores por ejemplo en el alojamiento de memoria en la GPU. En este caso es útil revisar:\n",
    "\n",
    "* [How to Query Device Properties and Handle Errors in CUDA C/C++](https://devblogs.nvidia.com/how-query-device-properties-and-handle-errors-cuda-cc/)\n",
    "\n",
    "En *stackoverflow* encontramos a personas desarrolladoras de CUDA que resuelven preguntas muy útiles para continuar con el aprendizaje de CUDA C. Por ejemplo: \n",
    "\n",
    "* [Parallel reduction over one axis](https://stackoverflow.com/questions/51526082/cuda-parallel-reduction-over-one-axis)\n",
    "\n",
    "Otros sistemas de software para el [Heterogeneous computing](https://en.wikipedia.org/wiki/Heterogeneous_computing) son:\n",
    "\n",
    "* [OpenCl](https://en.wikipedia.org/wiki/OpenCL). Ver [NVIDIA OpenCL SDK Code Samples](https://developer.nvidia.com/opencl) para ejemplos con NVIDIA GPU's.\n",
    "\n",
    "* [Rth-org/Rth](https://github.com/Rth-org/Rth) y más reciente [matloff/Rth](https://github.com/matloff/Rth). Ver también [rdrr.io matloff/Rth](https://rdrr.io/github/matloff/Rth/f/README.md).\n",
    "\n",
    "Es posible escribir [kernels](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/II.computo_paralelo/2.3.CUDA_C.ipynb) con CuPy. Ver por ejemplo: [User-Defined Kernels](https://docs-cupy.chainer.org/en/stable/tutorial/kernel.html).\n",
    "\n",
    "* [CuPy – NumPy-like API accelerated with CUDA](https://docs-cupy.chainer.org/en/stable/index.html)\n",
    "\n",
    "* [CuPy : NumPy-like API accelerated with CUDA github](https://github.com/cupy/cupy)\n",
    "\n",
    "Otro paquete para uso de Python+GPU para cómputo matricial es:\n",
    "\n",
    "* [PyCUDA](https://github.com/inducer/pycuda/) y ver [PyCUDA en el repo de la clase](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/tree/master/Python/PyCUDA) para más información.\n",
    "\n",
    "Un paquete para uso de pandas+GPU:\n",
    "\n",
    "* [Rapids](https://github.com/rapidsai), [cudf](https://github.com/rapidsai/cudf)\n",
    "\n",
    "Ver [optional-libraries](https://docs-cupy.chainer.org/en/stable/install.html#optional-libraries) para librerías que pueden ser utilizadas con CuPy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Ejercicios\n",
    ":class: tip\n",
    "\n",
    "1.Resuelve los ejercicios y preguntas de la nota.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas de comprehensión:**\n",
    "\n",
    "1)¿Qué factores han determinado un mejor *performance* de una GPU vs una CPU? (contrasta los diseños de una CPU vs una GPU).\n",
    "\n",
    "2)¿Dentro de qué modelo de arquitectura de máquinas se ubica a la GPU y que puede comparársele con el modelo **Single Program Multiple Data (SPMD)** dentro de la taxonomía de Flynn?\n",
    "\n",
    "3)¿Qué significan las siglas CUDA y detalla qué es CUDA?.\n",
    "\n",
    "4)¿Qué es y en qué consiste CUDA C?\n",
    "    \n",
    "5)¿Qué es un *kernel*?\n",
    "\n",
    "6)¿Qué pieza de CUDA se encarga de asignar los bloques de *cuda-threads* a las SM’s?\n",
    "\n",
    "7)¿Qué características (recursos compartidos, dimensiones, forma de agendar la ejecución en *threads*) tienen los bloques que se asignan a una SM al lanzarse y ejecutarse un *kernel*?\n",
    "\n",
    "8)¿Qué es un *warp*?\n",
    "\n",
    "9)Menciona los tipos de memorias que existen en las GPU’s.\n",
    "\n",
    "10) Supón que tienes una tarjeta GT200 cuyas características son:\n",
    "\n",
    "* Máximo número de *threads* que soporta una SM en un mismo instante en el tiempo: 1024\n",
    "* Máximo número de *threads* en un bloque: 512\n",
    "* Máximo número de bloques por SM: 8\n",
    "* Número de SM’s que tiene esta GPU: 30\n",
    "\n",
    "Responde:\n",
    "\n",
    "a)¿Cuál es la máxima cantidad de *threads* que puede soportar esta GPU en un mismo instante en el tiempo?\n",
    "b)¿Cuál es la máxima cantidad de *warps* por SM que puede soportar esta GPU en un mismo instante en el tiempo?\n",
    "c)¿Cuáles configuraciones de bloques y *threads* siguientes aprovechan la máxima cantidad de *warps* en una SM de esta GPU para un mismo instante en el tiempo?\n",
    "    \n",
    "1.Una configuración del tipo: bloques de 64 *threads* y 16 bloques.\n",
    "2.Una configuración del tipo: bloques de 1024 *threads* y 1 bloque.\n",
    "3.Una configuración del tipo: bloques de 256 *threads* y 4 bloques.\n",
    "4.Una configuración del tipo: bloques de 512 *threads* y 8 bloques.\n",
    "\n",
    "\\*Debes considerar las restricciones/características de la GPU dadas para responder pues algunas configuraciones infringen las mismas. No estamos considerando *registers* o *shared memory*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referencias:**\n",
    "\n",
    "1. N. Matloff, Parallel Computing for Data Science. With Examples in R, C++ and CUDA, 2014.\n",
    "\n",
    "4. [C/extensiones_a_C/CUDA/](https://github.com/palmoreck/programming-languages/tree/master/C/extensiones_a_C/CUDA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
