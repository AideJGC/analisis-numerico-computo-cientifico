{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(METDESCMASPRONUN)="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Método de descenso más pronunciado para *Unconstrained Convex Optimization* (UCO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Notas para contenedor de docker:\n",
    "\n",
    "Comando de docker para ejecución de la nota de forma local:\n",
    "\n",
    "nota: cambiar `<ruta a mi directorio>` por la ruta de directorio que se desea mapear a `/datos` dentro del contenedor de docker.\n",
    "\n",
    "`docker run --rm -v <ruta a mi directorio>:/datos --name jupyterlab_optimizacion_2 -p 8888:8888 -p 8787:8787 -d palmoreck/jupyterlab_optimizacion_2:3.0.0`\n",
    "\n",
    "password para jupyterlab: `qwerty`\n",
    "\n",
    "Detener el contenedor de docker:\n",
    "\n",
    "`docker stop jupyterlab_optimizacion_2`\n",
    "\n",
    "Documentación de la imagen de docker `palmoreck/jupyterlab_optimizacion_2:3.0.0` en [liga](https://github.com/palmoreck/dockerfiles/tree/master/jupyterlab/optimizacion_2).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Al final de esta nota la comunidad lectora:\n",
    ":class: tip\n",
    "\n",
    "*\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Perspectiva general del método de descenso más pronunciado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcular una dirección $\\Delta x_{\\text{nsd}}$ que satisfaga: \n",
    "\n",
    "$$\\Delta x_{\\text{nsd}} = \\text{argmin} \\{ \\nabla f_o(x)^Tv : ||v|| \\leq 1, \\nabla f_o(x)^Tv < 0 \\}$$ \n",
    "\n",
    "donde: $||\\cdot||$ es alguna norma en $\\mathbb{R}^n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente con $\\Delta x_{\\text{nsd}}$ se define el paso $\\Delta x_{\\text{sd}}=||\\nabla f_o(x)||_* \\Delta x_{\\text{nsd}}$ donde: $|| \\cdot||_*$ es la **norma dual**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definición\n",
    "\n",
    "La norma dual asociada a $||\\cdot||$ se denota como $||\\cdot||_*$ y se define como: \n",
    "\n",
    "$$||z||_* = \\sup \\{z^Tx :||x||= 1\\}.$$ \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentarios\n",
    "\n",
    "* Se puede probar que: $||z||_{2*}$ (la norma dual de la norma $2$) es $||z||_2$ y $||z||_{1*}$ (la norma dual de la norma $1$) es $||z||_\\infty$ $\\forall z \\in \\mathbb{R}^n$.\n",
    "\n",
    "* $\\Delta x_{\\text{sd}}$ es dirección de descenso.\n",
    "\n",
    "* En la definición de $\\Delta x_{\\text{nsd}}$ anterior:\n",
    "\n",
    "    * Si $||\\cdot||$ es la norma $2$: $\\Delta x_{\\text{nsd}} = \\text{argmin} \\{ \\nabla f_o(x)^Tv : ||v||_2 \\leq 1, \\nabla f_o(x)^Tv < 0 \\} $ entonces $\\Delta x_{\\text{sd}} = - \\nabla f_o(x)$. Con esto se prueba que el método de *steepest descent* generaliza al método de descenso en gradiente.\n",
    "\n",
    "    * Si $||\\cdot||$ es una norma cuadrática\\* con matriz $P$: $\\Delta x_{\\text{nsd}} = \\text{argmin} \\{ \\nabla f_o(x)^Tv : ||v||_P \\leq 1, \\nabla f_o(x)^Tv < 0 \\} $  se prueba que $\\Delta x_{\\text{sd}} = - P^{-1} \\nabla f_o(x)$.\n",
    "\n",
    "    * Si $||\\cdot||$ es la norma $1$: $\\Delta x_{\\text{nsd}} = \\text{argmin} \\{ \\nabla f_o(x)^Tv : ||v||_1 \\leq 1, \\nabla f_o(x)^Tv < 0 \\} $ se prueba que $\\Delta x_{\\text{sd}} = - \\frac{\\partial f(x)}{\\partial x_i} e_i$ con $e_i$ $i$-ésimo vector canónico y el índice $i$ es la entrada del vector $\\nabla f_o(x)$ de máxima magnitud: $i$ tal que $\\left |(\\nabla f_o(x))_i \\right | = ||\\nabla f_o(x)||_\\infty$. En este caso el método se nombra **descenso por coordenadas** o ***coordinate descent***. En cada iteración **una única** componente de $x$ es actualizada.\n",
    "    \n",
    "* **Interpretación:** $\\Delta x_{\\text{nsd}}$ es un paso tal que $||\\Delta x_{\\text{nsd}}|| = 1$ y da el **mayor decrecimiento en la aproximación lineal\\* de $f$**. Geométricamente es la dirección en la bola unitaria (generada por $||\\cdot||$) que se **extiende lo más lejos posible en la dirección $-\\nabla f(x)$**.\n",
    "\n",
    "\\*Recuérdese que la aproximación lineal de una función $f$ está dada por Taylor a primer orden: $f(x+v)=\\hat{f}(x+v) = f(x) + \\nabla f(x)^Tv$.\n",
    "\n",
    "* Para visualizar al paso $\\Delta x_{\\text{nsd}}$ se tiene el siguiente dibujo: \n",
    "\n",
    "**Con la norma cuadrática:**\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/91qivndznw2xfgb/steepest_descent_quadratic_norm.png?dl=0\" heigth=\"750\" width=\"750\">\n",
    "\n",
    "**Con la norma 1:**\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/qcbpuhvge9uqgim/steepest_desc_l1_norm.png?dl=0\" heigth=\"700\" width=\"700\">\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**De la nota descenso en gradiente_Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) $$\\min \\quad (x_1-2)^2+(2-x_2)^2+x_3^2+x_4^4$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) El método de descenso en gradiente es **altamente sensible** a la forma de las curvas de nivel de la función objetivo $f_o$. Para observar esto considérese el problema: $$\\min \\frac{1}{2}\\left(x_1^2+Cx_2^2 \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) $$\\min \\quad e^{(x_1+3x_2-0.1)}+e^{x_1-3x_2-0.1}+e^{-x_1-0.1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "* Como se observa en los ejemplos anteriores la elección de la norma en el método de descenso más pronunciado tiene un efecto fuerte en la tasa de convergencia. \n",
    "\n",
    "* Siempre existe una matriz $P$ para la cual el método de descenso más pronunciado tiene una convergencia buena. El reto está en encontrar tal matriz. La idea es identificar una matriz $P$ para la cual el problema transformado tenga un número de condición moderado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**De la nota descenso por coordenadas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) $$\\min \\quad (x_1-2)^2+(2-x_2)^2+x_3^2+x_4^4$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) $$\\min \\frac{1}{2}\\left(x_1^2+Cx_2^2 \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) $$\\min \\quad e^{(x_1+3x_2-0.1)}+e^{x_1-3x_2-0.1}+e^{-x_1-0.1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**De la nota Método Newton Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) $$\\min \\quad e^{(x_1+3x_2-0.1)}+e^{x_1-3x_2-0.1}+e^{-x_1-0.1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) El método de Newton **es insensible** a la forma de las curvas de nivel de la función objetivo $f_o$. Para observar esto considérese el problema: $$\\min \\frac{1}{2}\\left(x_1^2+Cx_2^2 \\right)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
