
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>5.5 Cómputo en paralelo usando GPUs en un sistema de memoria compartida (SMC)</title>
    
  <link rel="stylesheet" href="../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="6.1 Problemas tipo Unconstrained Convex Optimization (UCO)" href="../../VI.algoritmos_optimizacion_convexa/6.1/Problemas_UCO.html" />
    <link rel="prev" title="5.4 Cómputo en paralelo usando CPUs en un sistema de memoria compartida (SMC)" href="../5.4/Computo_en_paralelo_usando_CPUS_en_SMC.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/logo.png" class="logo" alt="logo">
  
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../README.html">
   Optimización
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  I. Cómputo científico
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../I.computo_cientifico/1.1/Analisis_numerico_y_computo_cientifico.html">
   1.1 Análisis numérico y cómputo científico
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../I.computo_cientifico/1.2/Sistema_de_punto_flotante.html">
   1.2 Sistema de punto flotante
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../I.computo_cientifico/1.3/Normas_vectoriales_y_matriciales.html">
   1.3 Normas vectoriales y matriciales
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../I.computo_cientifico/1.4/Condicion_de_un_problema_y_estabilidad_de_un_algoritmo.html">
   1.4 Condición de un problema y estabilidad de un algoritmo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../I.computo_cientifico/1.5/Definicion_de_funcion_continuidad_derivada.html">
   1.5 Definición de función, continuidad y derivada
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../I.computo_cientifico/1.6/Polinomios_de_Taylor_y_diferenciacion_numerica.html">
   1.6 Polinomios de Taylor y diferenciación numérica
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../I.computo_cientifico/1.7/Integracion_numerica.html">
   1.7 Integración Numérica
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  II. Cómputo matricial
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../II.computo_matricial/2.1/Operaciones_y_transformaciones_basicas_del_Algebra_Lineal_Numerica.html">
   2.1 Operaciones y transformaciones básicas del Álgebra Lineal Numérica
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../II.computo_matricial/2.2/Eigenvalores_y_eigenvectores.html">
   2.2 Eigenvalores y eigenvectores
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../II.computo_matricial/2.3/Algoritmos_y_aplicaciones_de_eigenvalores_eigenvectores_de_una_matriz.html">
   2.3 Algoritmos y aplicaciones de eigenvalores y eigenvectores de una matriz
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../II.computo_matricial/2.4/Valores_vectores_singulares_y_algoritmos_para_calcular_la_SVD.html">
   2.4 Valores, vectores singulares y algoritmos para calcular la SVD
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  III. Optimización convexa y ecuaciones no lineales
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../III.optimizacion_convexa/3.1/Definicion_de_problema_optimizacion_conjuntos_y_funciones_convexas.html">
   3.1 Definición de problemas de optimización, conjuntos y funciones convexas
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../III.optimizacion_convexa/3.2/Algoritmos_de_descenso_y_busqueda_de_linea_en_uco.html">
   3.2 Algoritmos de descenso y búsqueda de línea en
   <em>
    Unconstrained Convex Optimization
   </em>
   (UCO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../III.optimizacion_convexa/3.3/Ejemplos_problemas_UCO_e_intro_CIEO_y_PI.html">
   3.3 Ejemplos de problemas UCO, introducción a
   <em>
    Constrained Inequality and Equality Optimization
   </em>
   (CIEO) y puntos interiores
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../III.optimizacion_convexa/3.4/Ecuaciones_no_lineales.html">
   3.4 Ecuaciones no lineales
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  IV. Optimización en redes y programación lineal
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../IV.optimizacion_en_redes_y_prog_lineal/4.1/Definiciones_generales_de_flujo_en_redes.html">
   4.1 Definiciones generales de flujo en redes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../IV.optimizacion_en_redes_y_prog_lineal/4.2/Programacion_lineal_y_metodo_simplex.html">
   4.2 Programación lineal (PL) y método símplex
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../IV.optimizacion_en_redes_y_prog_lineal/4.3/Ejemplo_metodo_simplex_de_redes.html">
   4.3 Ejemplo del método símplex de redes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../IV.optimizacion_en_redes_y_prog_lineal/4.4/Dualidad_lema_de_Farkas_condiciones_KKT_de_optimalidad.html">
   4.4 Dualidad, lema de Farkas y condiciones de Karush-Kuhn-Tucker (KKT) de optimalidad
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../IV.optimizacion_en_redes_y_prog_lineal/4.5/Metodo_primal_dual_de_BL.html">
   4.5 Método primal-dual de barrera logarítmica (BL)
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  V. Optimización de código
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../5.1/introduccion_optimizacion_de_codigo.html">
   5.1 Introducción a optimización de código
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5.2/Herramientas_de_lenguajes_y_del_SO_para_perfilamiento_e_implementaciones_de_BLAS.html">
   5.2 Herramientas de lenguajes de programación y del sistema operativo para perfilamiento e implementaciones de BLAS
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5.3/Compilacion_a_C.html">
   5.3 Compilación a C
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5.4/Computo_en_paralelo_usando_CPUS_en_SMC.html">
   5.4 Cómputo en paralelo usando CPUs en un sistema de memoria compartida (SMC)
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   5.5 Cómputo en paralelo usando GPUs en un sistema de memoria compartida (SMC)
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  VI. Algoritmos de optimización convexa
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../VI.algoritmos_optimizacion_convexa/6.1/Problemas_UCO.html">
   6.1 Problemas tipo
   <em>
    Unconstrained Convex Optimization
   </em>
   (UCO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../VI.algoritmos_optimizacion_convexa/6.2/Problemas_CECO.html">
   6.2 Problemas tipo
   <em>
    Constrained Equality Convex Optimization
   </em>
   (CECO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../VI.algoritmos_optimizacion_convexa/6.3/Problemas_CIECO.html">
   6.3 Problemas tipo
   <em>
    Constrained Equality and Inequality Convex Optimization
   </em>
   (CIECO)
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/V.optimizacion_de_codigo/5.5/Computo_en_paralelo_usando_GPUS_en_SMC.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/palmoreck/dockerfiles-for-binder/jupyterlab_optimizacion_2?urlpath=lab/tree/analisis-numerico-computo-cientifico/libro_optimizacion/temas/V.optimizacion_de_codigo/5.5/Computo_en_paralelo_usando_GPUS_en_SMC.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#compute-unified-device-architecture-cuda">
   <em>
    Compute Unified Device Architecture
   </em>
   (CUDA)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#un-poco-de-historia">
     Un poco de historia…
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#diferencia-con-la-cpu-multicore">
     ¿Diferencia con la CPU multicore?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#otras-companias-producen-tarjetas-graficas">
     ¿Otras compañías producen tarjetas gráficas?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#si-tengo-una-tarjeta-grafica-de-amd-puedo-correr-un-programa-de-cuda">
     ¿Si tengo una tarjeta gráfica de AMD puedo correr un programa de CUDA?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#si-tengo-una-tarjeta-grafica-de-nvidia-un-poco-antigua-puedo-correr-un-programa-de-cuda">
     ¿Si tengo una tarjeta gráfica de NVIDIA un poco antigua puedo correr un programa de CUDA?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#que-es-cuda-c">
     ¿Qué es
     <em>
      CUDA C
     </em>
     ?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-que-se-refiere-la-terminologia-de-host-y-device">
     ¿A qué se refiere la terminología de
     <em>
      host
     </em>
     y
     <em>
      device
     </em>
     ?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tengo-una-tarjeta-nvidia-cuda-capable-que-debo-realizar-primero">
     Tengo una tarjeta NVIDIA CUDA
     <em>
      capable
     </em>
     ¿qué debo realizar primero?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#instale-lo-necesario-y-al-ejecutar-en-la-terminal-nvcc-v-obtengo-la-version-como-puedo-probar-mi-instalacion">
     Instalé lo necesario y al ejecutar en la terminal
     <code class="docutils literal notranslate">
      <span class="pre">
       nvcc
      </span>
      <span class="pre">
       -V
      </span>
     </code>
     obtengo la versión… ¿cómo puedo probar mi instalación?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#por-que-usar-cuda-y-cuda-c-o-mas-general-computo-en-la-gpu">
     ¿Por qué usar CUDA y
     <em>
      CUDA-C
     </em>
     o más general cómputo en la GPU?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cuda-c">
   CUDA-C
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kernel">
     Kernel
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ejemplo">
     Ejemplo
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bloques-de-threads">
     ¿Bloques de threads?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Ejemplo
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grid-s-y-bloques-3-dimensionales">
     ¿Grid’s y bloques 3-dimensionales?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Ejemplo
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     Ejemplo
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     Ejemplo
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     Ejemplo
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#alojamiento-de-memoria-en-el-device">
     Alojamiento de memoria en el
     <em>
      device
     </em>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     Ejemplo
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#perfilamiento-en-cuda">
     ¿Perfilamiento en CUDA?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tenemos-que-inicializar-los-datos-en-la-cpu-y-copiarlos-hacia-la-gpu">
     ¿Tenemos que inicializar los datos en la CPU y copiarlos hacia la GPU?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#arquitectura-de-una-gpu-y-limites-en-numero-de-threads-y-bloques-que-podemos-lanzar-en-el-kernel">
   Arquitectura de una GPU y límites en número de threads y bloques que podemos lanzar en el kernel
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cupy">
   CuPy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gputools">
   Gputools
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#referencias-de-interes">
   Referencias de interés
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="computo-en-paralelo-usando-gpus-en-un-sistema-de-memoria-compartida-smc">
<span id="compparalelogpussmc"></span><h1>5.5 Cómputo en paralelo usando GPUs en un sistema de memoria compartida (SMC)<a class="headerlink" href="#computo-en-paralelo-usando-gpus-en-un-sistema-de-memoria-compartida-smc" title="Permalink to this headline">¶</a></h1>
<div class="admonition-notas-para-contenedor-de-docker admonition">
<p class="admonition-title">Notas para contenedor de docker:</p>
<p>Comando de docker para ejecución de la nota de forma local:</p>
<p>nota: cambiar <code class="docutils literal notranslate"><span class="pre">&lt;ruta</span> <span class="pre">a</span> <span class="pre">mi</span> <span class="pre">directorio&gt;</span></code> por la ruta de directorio que se desea mapear a <code class="docutils literal notranslate"><span class="pre">/datos</span></code> dentro del contenedor de docker.</p>
<p><code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span> <span class="pre">--rm</span> <span class="pre">-v</span> <span class="pre">&lt;ruta</span> <span class="pre">a</span> <span class="pre">mi</span> <span class="pre">directorio&gt;:/datos</span> <span class="pre">--name</span> <span class="pre">jupyterlab_optimizacion_2</span> <span class="pre">-p</span> <span class="pre">8888:8888</span> <span class="pre">-p</span> <span class="pre">8787:8787</span> <span class="pre">-d</span> <span class="pre">palmoreck/jupyterlab_optimizacion_2:3.0.0</span></code></p>
<p>password para jupyterlab: <code class="docutils literal notranslate"><span class="pre">qwerty</span></code></p>
<p>Detener el contenedor de docker:</p>
<p><code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">stop</span> <span class="pre">jupyterlab_optimizacion_2</span></code></p>
<p>Documentación de la imagen de docker <code class="docutils literal notranslate"><span class="pre">palmoreck/jupyterlab_optimizacion_2:3.0.0</span></code> en <a class="reference external" href="https://github.com/palmoreck/dockerfiles/tree/master/jupyterlab/optimizacion_2">liga</a>.</p>
</div>
<hr class="docutils" />
<p>Nota generada a partir de <a class="reference external" href="https://www.dropbox.com/s/yjijtfuky3s5dfz/2.5.Compute_Unified_Device_Architecture.pdf?dl=0">liga</a></p>
<div class="tip admonition">
<p class="admonition-title">Al final de esta nota el y la lectora:</p>
<ul class="simple">
<li></li>
</ul>
</div>
<p>Se presentan códigos y sus ejecuciones en una máquina <code class="docutils literal notranslate"><span class="pre">p2.xlarge</span></code> de la nube de <a class="reference external" href="https://aws.amazon.com/">AWS</a>. Se utilizó la AMI <code class="docutils literal notranslate"><span class="pre">opt2-aws-cuda-and-tools-08-05-2021</span></code> de la región <code class="docutils literal notranslate"><span class="pre">us-east-1</span></code> (Virginia) para reproducibilidad de resultados. Tal AMI se construyó a partir de una AMI <code class="docutils literal notranslate"><span class="pre">ubuntu</span> <span class="pre">20.04</span> <span class="pre">-</span> <span class="pre">ami-042e8287309f5df03</span></code> con el <a class="reference external" href="https://github.com/palmoreck/scripts_for_useful_tools_installations/blob/main/AWS/ubuntu_20.04/optimizacion_2/script_cuda_and_tools.sh">script_cuda_and_tools.sh</a></p>
<div class="admonition-comentario admonition">
<p class="admonition-title">Comentario</p>
<p>Si se utiliza la <em>AMI</em> <code class="docutils literal notranslate"><span class="pre">opt2-aws-cuda-and-tools-08-05-2021</span></code> colocar en <code class="docutils literal notranslate"><span class="pre">User</span> <span class="pre">data</span></code> el siguiente <em>script</em>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">##variables:</span>
<span class="nv">region</span><span class="o">=</span>us-east-1 <span class="c1">#make sure instance is in Virginia</span>
<span class="nv">name_instance</span><span class="o">=</span>CUDA
<span class="nv">USER</span><span class="o">=</span>ubuntu
<span class="c1">##System update</span>
apt-get update -yq
<span class="c1">##Tag instance</span>
<span class="nv">INSTANCE_ID</span><span class="o">=</span><span class="k">$(</span>curl -s http://instance-data/latest/meta-data/instance-id<span class="k">)</span>
<span class="nv">PUBLIC_IP</span><span class="o">=</span><span class="k">$(</span>curl -s http://instance-data/latest/meta-data/public-ipv4<span class="k">)</span>
sudo -H -u <span class="nv">$USER</span> bash -c <span class="s2">&quot;/home/</span><span class="nv">$USER</span><span class="s2">/.local/bin/aws ec2 create-tags --resources </span><span class="nv">$INSTANCE_ID</span><span class="s2"> --tag Key=Name,Value=</span><span class="nv">$name_instance</span><span class="s2">-</span><span class="nv">$PUBLIC_IP</span><span class="s2"> --region=</span><span class="nv">$region</span><span class="s2">&quot;</span>
sudo -H -u <span class="nv">$USER</span> bash -c <span class="s2">&quot;cd / &amp;&amp; /home/</span><span class="nv">$USER</span><span class="s2">/.local/bin/jupyter lab --ip=0.0.0.0 --no-browser --config=/home/</span><span class="nv">$USER</span><span class="s2">/.jupyter/jupyter_notebook_config.py &amp;&quot;</span>
</pre></div>
</div>
</div>
<p>La máquina <code class="docutils literal notranslate"><span class="pre">p2.xlarge</span></code> tiene las siguientes características:</p>
<p><strong>Falta ejecutar de acuerdo a la máquina elegida</strong></p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
lscpu
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   46 bits physical, 48 bits virtual
CPU(s):                          4
On-line CPU(s) list:             0-3
Thread(s) per core:              2
Core(s) per socket:              2
Socket(s):                       1
NUMA node(s):                    1
Vendor ID:                       GenuineIntel
CPU family:                      6
Model:                           79
Model name:                      Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz
Stepping:                        1
CPU MHz:                         2701.377
CPU max MHz:                     3000.0000
CPU min MHz:                     1200.0000
BogoMIPS:                        4600.15
Hypervisor vendor:               Xen
Virtualization type:             full
L1d cache:                       64 KiB
L1i cache:                       64 KiB
L2 cache:                        512 KiB
L3 cache:                        45 MiB
NUMA node0 CPU(s):               0-3
Vulnerability Itlb multihit:     KVM: Vulnerable
Vulnerability L1tf:              Mitigation; PTE Inversion
Vulnerability Mds:               Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown
Vulnerability Meltdown:          Mitigation; PTI
Vulnerability Spec store bypass: Vulnerable
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Full generic retpoline, STIBP disabled, RSB filling
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx xsaveopt
</pre></div>
</div>
</div>
</div>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
sudo lshw -C memory
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  *-firmware
       description: BIOS
       vendor: Xen
       physical id: 0
       version: 4.2.amazon
       date: 08/24/2006
       size: 96KiB
       capabilities: pci edd
  *-memory
       description: System Memory
       physical id: 1000
       size: 61GiB
       capabilities: ecc
       configuration: errordetection=multi-bit-ecc
     *-bank:0
          description: DIMM RAM
          physical id: 0
          slot: DIMM 0
          size: 16GiB
          width: 64 bits
     *-bank:1
          description: DIMM RAM
          physical id: 1
          slot: DIMM 1
          size: 16GiB
          width: 64 bits
     *-bank:2
          description: DIMM RAM
          physical id: 2
          slot: DIMM 2
          size: 16GiB
          width: 64 bits
     *-bank:3
          description: DIMM RAM
          physical id: 3
          slot: DIMM 3
          size: 13GiB
          width: 64 bits
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
uname -ar #r for kernel, a for all
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Linux ip-10-0-0-128 5.4.0-1045-aws #47-Ubuntu SMP Tue Apr 13 07:02:25 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
</pre></div>
</div>
</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Observación</p>
<p>En la celda anterior se utilizó el comando de <em>magic</em> <code class="docutils literal notranslate"><span class="pre">%%bash</span></code>. Algunos comandos de <em>magic</em> los podemos utilizar también con <code class="docutils literal notranslate"><span class="pre">import</span></code>. Ver <a class="reference external" href="https://ipython.readthedocs.io/en/stable/interactive/magics.html">ipython-magics</a></p>
</div>
<div class="section" id="compute-unified-device-architecture-cuda">
<h2><em>Compute Unified Device Architecture</em> (CUDA)<a class="headerlink" href="#compute-unified-device-architecture-cuda" title="Permalink to this headline">¶</a></h2>
<div class="section" id="un-poco-de-historia">
<h3>Un poco de historia…<a class="headerlink" href="#un-poco-de-historia" title="Permalink to this headline">¶</a></h3>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>GPGPU es un término que se utilizó para referirse a la programación general en unidades de procesamiento gráfico. Hoy en día se conoce simplemente como <em>GPU programming</em>. Ver <a class="reference external" href="https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units">General-purpose computing on graphics processing units</a>.</p>
</div>
<p>La industria de videojuegos impulsó el desarrollo de las tarjetas gráficas a una velocidad sin precedente a partir del año 1999 para incrementar el nivel de detalle visual en los juegos de video. Alrededor del 2003 se planteó la posibilidad de utilizar las unidades de procesamiento gráfico para procesamiento en paralelo relacionado con aplicaciones distintas al ambiente de gráficas. A partir del 2006 la empresa <a class="reference external" href="https://www.nvidia.com/en-us/about-nvidia/">NVIDIA</a> introdujo CUDA, una plataforma GPGPU y un modelo de programación que facilita el procesamiento en paralelo en las GPU’s.</p>
<p>Desde el 2006, las tarjetas gráficas han creado una brecha significativa con las unidades de procesamiento, CPU’s. Ver por ejemplo las gráficas que <em>NVIDIA</em> publica año tras año y que están relacionadas con el número de operaciones en punto flotante por segundo (FLOPS) y la transferencia de datos en la memoria RAM de la GPU: <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#from-graphics-processing-to-general-purpose-parallel-computing">from-graphics-processing-to-general-purpose-parallel-computing</a>.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>La GPU y la CPU están conectadas por una interconexión de nombre <a class="reference external" href="https://en.wikipedia.org/wiki/Conventional_PCI">PCI</a>.</p>
</div>
<p>Hoy en día se continúa el desarrollo de GPU’s con mayor RAM, con mayor capacidad de cómputo y mejor conectividad con la CPU. Estos avances han permitido resolver problemas con mejor exactitud que los resueltos con las CPU’s, por ejemplo en el terreno de <em>deep learning</em> en reconocimiento de imágenes. Ver <a class="reference external" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a>, <a class="reference external" href="https://medium.com/limitlessai/2012-a-breakthrough-year-for-deep-learning-2a31a6796e73">2012: A Breakthrough Year for Deep Learning</a>.</p>
<div class="tip admonition">
<p class="admonition-title">Observación</p>
<p>Para avances al día de hoy ver <a class="reference external" href="https://devblogs.nvidia.com/nvidia-turing-architecture-in-depth/">NVIDIA Turing Architecture In-Depth</a>, <a class="reference external" href="https://wccftech.com/samsung-amd-rdna-gpu-2021/">samsung-amd-rdna-gpu-2021</a>, <a class="reference external" href="https://www.theguardian.com/games/2020/mar/19/playstation-5-specifications-revealed-but-design-is-still-a-mystery">playstation-5-specifications-revealed-but-design-is-still-a-mystery</a>, <a class="reference external" href="https://news.xbox.com/en-us/2020/03/16/xbox-series-x-tech/">xbox-series-x-tech</a> y recientemente <a class="reference external" href="https://www.ibm.com/blogs/nordic-msp/ibm-supercomputer-summit-attacks-coronavirus/">IBM Supercomputer Summit Attacks Coronavirus…</a>.</p>
</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Recuérdese la <a class="reference external" href="https://en.wikipedia.org/wiki/Flynn%27s_taxonomy">taxonomía de Flynn</a>.</p>
</div>
<p>La arquitectura en la que podemos ubicar a las GPU’s es en la de un sistema MIMD y SIMD. De hecho es <a class="reference external" href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads">SIMT: Simple Instruction Multiple Thread</a> en un modelo de sistema de memoria compartida pues “los <em>threads</em> en un <em>warp</em> leen la misma instrucción para ser ejecutada”.</p>
<div class="admonition-definicion admonition">
<p class="admonition-title">Definición</p>
<p>Un <em>warp</em> en el contexto de GPU <em>programming</em> es un conjunto de <em>threads</em>. Equivale a <span class="math notranslate nohighlight">\(32\)</span> <em>threads</em>.</p>
</div>
</div>
<div class="section" id="diferencia-con-la-cpu-multicore">
<h3>¿Diferencia con la CPU multicore?<a class="headerlink" href="#diferencia-con-la-cpu-multicore" title="Permalink to this headline">¶</a></h3>
<img src="https://dl.dropboxusercontent.com/s/k11qub01w4nvksi/CPU_multicore.png?dl=0" heigth="500" width="500">
<p><strong>GPU</strong></p>
<img src="https://dl.dropboxusercontent.com/s/lw9kia12qhwp95r/GPU.png?dl=0" heigth="500" width="500"><div class="tip admonition">
<p class="admonition-title">Observación</p>
<p>Obsérvese en el dibujo anterior la diferencia en tamaño del caché en la CPU y GPU. También la unidad de control es más pequeña en la GPU.</p>
</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Una máquina <em>quad core</em> soporta cuatro threads en cada <em>core</em>.</p>
</div>
<p>A diferencia de una máquina <em>multicore</em> o multi CPU’s con la habilidad de lanzar en un instante de tiempo unos cuantos <em>threads</em>, la GPU puede lanzar cientos o miles de threads en un instante siendo cada core <em>heavily multithreaded</em>. Sí hay restricciones en el número de threads que se pueden lanzar en un instante pues las tarjetas gráficas tienen diferentes características (modelo) y arquitecturas, pero la diferencia es grande. Por ejemplo, la serie <strong>GT 200</strong> (2009) en un instante puede lanzar 30,720 threads en sus 240 <em>cores</em>. Ver <a class="reference external" href="https://en.wikipedia.org/wiki/GeForce_200_series">GeForce_200_series</a>, <a class="reference external" href="https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units">List of NVIDIA GPU’s</a>.</p>
<p>Ver <a class="reference external" href="https://computer.howstuffworks.com/graphics-card1.htm">How Graphics Cards Work</a> y <a class="reference external" href="https://computer.howstuffworks.com/microprocessor.htm">How Microprocessors Work</a> para más información.</p>
</div>
<div class="section" id="otras-companias-producen-tarjetas-graficas">
<h3>¿Otras compañías producen tarjetas gráficas?<a class="headerlink" href="#otras-companias-producen-tarjetas-graficas" title="Permalink to this headline">¶</a></h3>
<p>Ver por ejemplo la lista de GPU’s de <a class="reference external" href="https://en.wikipedia.org/wiki/List_of_AMD_graphics_processing_units">Advanced Micro Devices</a></p>
</div>
<div class="section" id="si-tengo-una-tarjeta-grafica-de-amd-puedo-correr-un-programa-de-cuda">
<h3>¿Si tengo una tarjeta gráfica de AMD puedo correr un programa de CUDA?<a class="headerlink" href="#si-tengo-una-tarjeta-grafica-de-amd-puedo-correr-un-programa-de-cuda" title="Permalink to this headline">¶</a></h3>
<p>No es posible pero entre las alternativas están:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.khronos.org/opencl/">OpenCl</a></p></li>
<li><p><a class="reference external" href="https://www.openacc.org/about">OpenACC</a></p></li>
</ul>
</div>
<div class="section" id="si-tengo-una-tarjeta-grafica-de-nvidia-un-poco-antigua-puedo-correr-un-programa-de-cuda">
<h3>¿Si tengo una tarjeta gráfica de NVIDIA un poco antigua puedo correr un programa de CUDA?<a class="headerlink" href="#si-tengo-una-tarjeta-grafica-de-nvidia-un-poco-antigua-puedo-correr-un-programa-de-cuda" title="Permalink to this headline">¶</a></h3>
<p>Las GPU’s producidas por NVIDIA desde 2006 son capaces de correr programas basados en <em><strong>CUDA C</strong></em>. La cuestión sería revisar qué <em>compute capability</em> tiene tu tarjeta. Ver <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities">Compute Capabilities</a> para las características que tienen las tarjetas más actuales.</p>
</div>
<div class="section" id="que-es-cuda-c">
<h3>¿Qué es <em>CUDA C</em>?<a class="headerlink" href="#que-es-cuda-c" title="Permalink to this headline">¶</a></h3>
<p>Es una extensión al lenguaje <em>C</em> de programación en el que se utiliza una nueva sintaxis para procesamiento en la GPU. Contiene también una librería <em>runtime</em> que define funciones que se ejecutan desde el <em><strong>host</strong></em> por ejemplo para alojar y desalojar memoria en el <em><strong>device</strong></em>, transferir datos entre la memoria <em>host</em> y la memoria <em>device</em> o manejar múltiples devices. La librería <em>runtime</em> está hecha encima de una API de <em>C</em> de bajo nivel llamada <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-driver-api/index.html">NVIDIA CUDA Driver API</a> la cual es accesible desde el código. Para información de la API de la librería runtime ver <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/index.html">NVIDIA CUDA Runtime API</a>.</p>
<div class="admonition-comentario admonition">
<p class="admonition-title">Comentario</p>
<p>La transferencia de datos entre la memoria del <em>host</em> a <em>device</em> o viceversa es un <em>bottleneck</em> fuerte.</p>
</div>
</div>
<div class="section" id="a-que-se-refiere-la-terminologia-de-host-y-device">
<h3>¿A qué se refiere la terminología de <em>host</em> y <em>device</em>?<a class="headerlink" href="#a-que-se-refiere-la-terminologia-de-host-y-device" title="Permalink to this headline">¶</a></h3>
<p><em>Host</em> es la máquina <em>multicore</em> CPU y <em>device</em> es la GPU. Una máquina puede tener múltiples GPU’s por lo que tendrá múltiples <em>devices</em>.</p>
</div>
<div class="section" id="tengo-una-tarjeta-nvidia-cuda-capable-que-debo-realizar-primero">
<h3>Tengo una tarjeta NVIDIA CUDA <em>capable</em> ¿qué debo realizar primero?<a class="headerlink" href="#tengo-una-tarjeta-nvidia-cuda-capable-que-debo-realizar-primero" title="Permalink to this headline">¶</a></h3>
<p>Realizar instalaciones dependiendo de tu sistema operativo. Ver <a class="reference external" href="https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/tree/master/C/extensiones_a_C/CUDA/instalacion">Instalación</a> donde además se encontrará información para instalación de <a class="reference external" href="https://github.com/NVIDIA/nvidia-docker">nvidia-docker</a>.</p>
</div>
<div class="section" id="instale-lo-necesario-y-al-ejecutar-en-la-terminal-nvcc-v-obtengo-la-version-como-puedo-probar-mi-instalacion">
<h3>Instalé lo necesario y al ejecutar en la terminal <code class="docutils literal notranslate"><span class="pre">nvcc</span> <span class="pre">-V</span></code> obtengo la versión… ¿cómo puedo probar mi instalación?<a class="headerlink" href="#instale-lo-necesario-y-al-ejecutar-en-la-terminal-nvcc-v-obtengo-la-version-como-puedo-probar-mi-instalacion" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>Obteniendo información del NVIDIA driver ejecutando en la terminal.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
nvidia-smi
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sun May  9 00:02:02 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA Tesla K80    On   | 00000000:00:1E.0 Off |                    0 |
| N/A   41C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</pre></div>
</div>
</div>
</div>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
nvidia-smi -a
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>==============NVSMI LOG==============

Timestamp                                 : Sun May  9 00:02:03 2021
Driver Version                            : 465.19.01
CUDA Version                              : 11.3

Attached GPUs                             : 1
GPU 00000000:00:1E.0
    Product Name                          : NVIDIA Tesla K80
    Product Brand                         : Tesla
    Display Mode                          : Disabled
    Display Active                        : Disabled
    Persistence Mode                      : Enabled
    MIG Mode
        Current                           : N/A
        Pending                           : N/A
    Accounting Mode                       : Disabled
    Accounting Mode Buffer Size           : 4000
    Driver Model
        Current                           : N/A
        Pending                           : N/A
    Serial Number                         : 0321417072356
    GPU UUID                              : GPU-c2ec0b4b-de80-2dfc-d38a-1b7a8b1dd8aa
    Minor Number                          : 0
    VBIOS Version                         : 80.21.1F.00.01
    MultiGPU Board                        : No
    Board ID                              : 0x1e
    GPU Part Number                       : 900-22080-0000-000
    Inforom Version
        Image Version                     : 2080.0200.00.04
        OEM Object                        : 1.1
        ECC Object                        : 3.0
        Power Management Object           : N/A
    GPU Operation Mode
        Current                           : N/A
        Pending                           : N/A
    GPU Virtualization Mode
        Virtualization Mode               : Pass-Through
        Host VGPU Mode                    : N/A
    IBMNPU
        Relaxed Ordering Mode             : N/A
    PCI
        Bus                               : 0x00
        Device                            : 0x1E
        Domain                            : 0x0000
        Device Id                         : 0x102D10DE
        Bus Id                            : 00000000:00:1E.0
        Sub System Id                     : 0x106C10DE
        GPU Link Info
            PCIe Generation
                Max                       : 3
                Current                   : 1
            Link Width
                Max                       : 16x
                Current                   : 16x
        Bridge Chip
            Type                          : N/A
            Firmware                      : N/A
        Replays Since Reset               : 0
        Replay Number Rollovers           : 0
        Tx Throughput                     : N/A
        Rx Throughput                     : N/A
    Fan Speed                             : N/A
    Performance State                     : P8
    Clocks Throttle Reasons
        Idle                              : Active
        Applications Clocks Setting       : Not Active
        SW Power Cap                      : Not Active
        HW Slowdown                       : Not Active
            HW Thermal Slowdown           : N/A
            HW Power Brake Slowdown       : N/A
        Sync Boost                        : Not Active
        SW Thermal Slowdown               : Not Active
        Display Clock Setting             : Not Active
    FB Memory Usage
        Total                             : 11441 MiB
        Used                              : 0 MiB
        Free                              : 11441 MiB
    BAR1 Memory Usage
        Total                             : 16384 MiB
        Used                              : 2 MiB
        Free                              : 16382 MiB
    Compute Mode                          : Default
    Utilization
        Gpu                               : 0 %
        Memory                            : 0 %
        Encoder                           : 0 %
        Decoder                           : 0 %
    Encoder Stats
        Active Sessions                   : 0
        Average FPS                       : 0
        Average Latency                   : 0
    FBC Stats
        Active Sessions                   : 0
        Average FPS                       : 0
        Average Latency                   : 0
    Ecc Mode
        Current                           : Enabled
        Pending                           : Enabled
    ECC Errors
        Volatile
            Single Bit            
                Device Memory             : 0
                Register File             : 0
                L1 Cache                  : 0
                L2 Cache                  : 0
                Texture Memory            : 0
                Texture Shared            : N/A
                CBU                       : N/A
                Total                     : 0
            Double Bit            
                Device Memory             : 0
                Register File             : 0
                L1 Cache                  : 0
                L2 Cache                  : 0
                Texture Memory            : 0
                Texture Shared            : N/A
                CBU                       : N/A
                Total                     : 0
        Aggregate
            Single Bit            
                Device Memory             : 3
                Register File             : 0
                L1 Cache                  : 0
                L2 Cache                  : 0
                Texture Memory            : 0
                Texture Shared            : N/A
                CBU                       : N/A
                Total                     : 3
            Double Bit            
                Device Memory             : 0
                Register File             : 0
                L1 Cache                  : 0
                L2 Cache                  : 0
                Texture Memory            : 0
                Texture Shared            : N/A
                CBU                       : N/A
                Total                     : 0
    Retired Pages
        Single Bit ECC                    : 1
        Double Bit ECC                    : 0
        Pending Page Blacklist            : No
    Remapped Rows                         : N/A
    Temperature
        GPU Current Temp                  : 41 C
        GPU Shutdown Temp                 : 110 C
        GPU Slowdown Temp                 : 88 C
        GPU Max Operating Temp            : N/A
        GPU Target Temperature            : N/A
        Memory Current Temp               : N/A
        Memory Max Operating Temp         : N/A
    Power Readings
        Power Management                  : Supported
        Power Draw                        : 27.49 W
        Power Limit                       : 149.00 W
        Default Power Limit               : 149.00 W
        Enforced Power Limit              : 149.00 W
        Min Power Limit                   : 100.00 W
        Max Power Limit                   : 175.00 W
    Clocks
        Graphics                          : 324 MHz
        SM                                : 324 MHz
        Memory                            : 324 MHz
        Video                             : 405 MHz
    Applications Clocks
        Graphics                          : 562 MHz
        Memory                            : 2505 MHz
    Default Applications Clocks
        Graphics                          : 562 MHz
        Memory                            : 2505 MHz
    Max Clocks
        Graphics                          : 875 MHz
        SM                                : 875 MHz
        Memory                            : 2505 MHz
        Video                             : 540 MHz
    Max Customer Boost Clocks
        Graphics                          : N/A
    Clock Policy
        Auto Boost                        : On
        Auto Boost Default                : On
    Processes                             : None
</pre></div>
</div>
</div>
</div>
<ol class="simple">
<li><p>Compilando y ejecutando el siguiente programa de <em>CUDA C</em>:</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%file</span> hello_world.cu

<span class="c1">#include&lt;stdio.h&gt;</span>
<span class="n">__global__</span> <span class="n">void</span> <span class="n">func</span><span class="p">(</span><span class="n">void</span><span class="p">){</span>
    <span class="n">printf</span><span class="p">(</span><span class="s2">&quot;Hello world! del bloque </span><span class="si">%d</span><span class="s2"> del thread </span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">blockIdx</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span><span class="p">);</span>
<span class="p">}</span>
<span class="nb">int</span> <span class="n">main</span><span class="p">(</span><span class="n">void</span><span class="p">){</span>
    <span class="n">func</span><span class="o">&lt;&lt;&lt;</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="o">&gt;&gt;&gt;</span><span class="p">();</span>
    <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
    <span class="n">printf</span><span class="p">(</span><span class="s2">&quot;Hello world! del cpu thread</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">);</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Writing hello_world.cu
</pre></div>
</div>
</div>
</div>
<div class="admonition-comentario admonition">
<p class="admonition-title">Comentario</p>
<p>La sintaxis <code class="docutils literal notranslate"><span class="pre">&lt;&lt;&lt;2,3&gt;&gt;&gt;</span></code> representa 2 bloques de 3 <em>threads</em> cada uno.</p>
</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Ver <a class="reference external" href="https://stackoverflow.com/questions/63675040/cuda-11-kernel-doesnt-run">cuda-11-kernel-doesnt-run</a>, <a class="reference external" href="https://stackoverflow.com/questions/35656294/cuda-how-to-use-arch-and-code-and-sm-vs-compute/35657430#35657430">cuda-how-to-use-arch-and-code-and-sm-vs-compute</a>, <a class="reference external" href="https://stackoverflow.com/questions/28932864/cuda-compute-capability-requirements/28933055#28933055">cuda-compute-capability-requirements</a>, <a class="reference external" href="https://stackoverflow.com/questions/14038589/what-is-the-canonical-way-to-check-for-errors-using-the-cuda-runtime-api">what-is-the-canonical-way-to-check-for-errors-using-the-cuda-runtime-api</a></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
source ~/.profile
nvcc -gencode arch=compute_37,code=sm_37 hello_world.cu -o hello_world.out
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>nvcc warning : The &#39;compute_35&#39;, &#39;compute_37&#39;, &#39;compute_50&#39;, &#39;sm_35&#39;, &#39;sm_37&#39; and &#39;sm_50&#39; architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
./hello_world.out
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Hello world! del bloque 0 del thread 0
Hello world! del bloque 0 del thread 1
Hello world! del bloque 0 del thread 2
Hello world! del bloque 1 del thread 0
Hello world! del bloque 1 del thread 1
Hello world! del bloque 1 del thread 2
Hello world! del cpu thread
</pre></div>
</div>
</div>
</div>
<ol class="simple">
<li><p>Haciendo un query a la GPU para ver qué características tiene (lo siguiente es posible ejecutar sólo si se instaló el <em>CUDA toolkit</em>):</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
cd /usr/local/cuda/samples/1_Utilities/deviceQuery/ &amp;&amp; sudo make
/usr/local/cuda/samples/1_Utilities/deviceQuery/deviceQuery
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/cuda-11.3/bin/nvcc -ccbin g++   -m64      -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -o deviceQuery deviceQuery.o 
mkdir -p ../../bin/x86_64/linux/release
cp deviceQuery ../../bin/x86_64/linux/release
/usr/local/cuda/samples/1_Utilities/deviceQuery/deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 1 CUDA Capable device(s)

Device 0: &quot;NVIDIA Tesla K80&quot;
  CUDA Driver Version / Runtime Version          11.3 / 11.3
  CUDA Capability Major/Minor version number:    3.7
  Total amount of global memory:                 11441 MBytes (11996954624 bytes)
  (013) Multiprocessors, (192) CUDA Cores/MP:    2496 CUDA Cores
  GPU Max Clock rate:                            824 MHz (0.82 GHz)
  Memory Clock rate:                             2505 Mhz
  Memory Bus Width:                              384-bit
  L2 Cache Size:                                 1572864 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)
  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total shared memory per multiprocessor:        114688 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)
  Run time limit on kernels:                     No
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Enabled
  Device supports Unified Addressing (UVA):      Yes
  Device supports Managed Memory:                Yes
  Device supports Compute Preemption:            No
  Supports Cooperative Kernel Launch:            No
  Supports MultiDevice Co-op Kernel Launch:      No
  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 30
  Compute Mode:
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.3, CUDA Runtime Version = 11.3, NumDevs = 1
Result = PASS
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>nvcc warning : The &#39;compute_35&#39;, &#39;compute_37&#39;, &#39;compute_50&#39;, &#39;sm_35&#39;, &#39;sm_37&#39; and &#39;sm_50&#39; architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="por-que-usar-cuda-y-cuda-c-o-mas-general-computo-en-la-gpu">
<h3>¿Por qué usar CUDA y <em>CUDA-C</em> o más general cómputo en la GPU?<a class="headerlink" href="#por-que-usar-cuda-y-cuda-c-o-mas-general-computo-en-la-gpu" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>NVIDIA como se mencionó al inicio de la nota fue de las primeras compañías en utilizar la GPU para tareas no relacionadas con el área de gráficos y ha colaborado en el avance del conocimiento de las GPU’s y desarrollo de algoritmos y tarjetas gráficas. Otra compañía es <a class="reference external" href="https://en.wikipedia.org/wiki/Khronos_Group">Khronos_Group</a> por ejemplo, quien actualmente desarrolla <a class="reference external" href="https://www.khronos.org/opencl/">OpenCl</a>.</p></li>
</ul>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p><em>Deep learning</em> se ha utilizado para resolver problemas en <em>machine learning</em> típicos. Ejemplos de esto son la clasificación de imágenes, de sonidos o análisis de textos, ver por ejemplo <a class="reference external" href="https://medium.com/&#64;michael.fire/practical-text-analysis-using-deep-learning-5fb0744efdf9">Practical text analysis using deep learning</a>.</p>
</div>
<ul class="simple">
<li><p>El cómputo en la GPU constituye hoy en día (2020) una alternativa fuerte a la implementación de modelos de machine learning ampliamente utilizada por la comunidad científica. En el terreno de cómputo matricial y <em>deep learning</em> se encuentran <a class="reference external" href="https://github.com/tensorflow">tensorflow</a>, <a class="reference external" href="https://github.com/BVLC/caffe">caffe</a>, <a class="reference external" href="https://github.com/Theano/Theano">Theano</a> (aunque ya no es soportado pero está retomado en <a class="reference external" href="https://github.com/pymc-devs/aesara">pymc-devs/aesara</a>), <a class="reference external" href="https://github.com/pytorch/pytorch">Pytorch</a> y <a class="reference external" href="https://github.com/keras-team/keras">keras</a> como ejemplos de lo anterior.</p></li>
<li><p>Sí hay publicaciones científicas para la implementación de <em>deep learning</em> en las CPU’s, ver por ejemplo el paper reciente de <a class="reference external" href="https://www.cs.rice.edu/~as143/Papers/SLIDE_MLSys.pdf">SLIDE</a>, <a class="reference external" href="https://github.com/keroro824/HashingDeepLearning">HashingDeepLearning</a> y las entradas <a class="reference external" href="https://www.engadget.com/2020/03/03/rice-university-slide-cpu-gpu-machine-learning/">An algorithm could make CPUs a cheap way to train AI</a> y <a class="reference external" href="https://www.sciencedaily.com/releases/2020/03/200305135041.htm">Deep learning rethink overcomes major obstacle in AI industry</a> . Sin embargo, esta área se encuentra en activa investigación por lo que se han adoptado el uso de implementaciones del <em>deep learning</em> utilizando GPU’s.</p></li>
</ul>
<div class="admonition-comentario admonition">
<p class="admonition-title">Comentario</p>
<p>El paper plantea una discusión a realizar con la frase <em>…change in the state-of-the-art algorithms can render specialized hardware less effective in the future</em>. Ver por ejemplo <a class="reference external" href="https://developer.nvidia.com/tensor-cores">Tensor Cores</a>, <a class="reference external" href="https://www.nvidia.com/en-us/data-center/tensorcore/">NVIDIA TENSOR CORES, The Next Generation of Deep Learning</a>, <a class="reference external" href="https://www.ibm.com/thought-leadership/summit-supercomputer/">The most powerful computers on the planet: SUMMIT</a> como ejemplos de hardware especializado en aprendizaje automático con Tensorflow.</p>
<p><em>Summit powered by 9,126 IBM Power9 CPUs and over 27,000 NVIDIA V100 Tensor Core GPUS, is able to do 200 quadrillion calculations per second…</em> <a class="reference external" href="https://www.ibm.com/blogs/nordic-msp/ibm-supercomputer-summit-attacks-coronavirus/">IBM Supercomputer Summit Attacks Coronavirus…</a>.</p>
</div>
</div>
</div>
<div class="section" id="cuda-c">
<h2><a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/">CUDA-C</a><a class="headerlink" href="#cuda-c" title="Permalink to this headline">¶</a></h2>
<p>Consiste en extensiones al lenguaje C y en una <em>runtime library</em>.</p>
<div class="section" id="kernel">
<h3>Kernel<a class="headerlink" href="#kernel" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>En CUDA C se define una función que se ejecuta en el <em><strong>device</strong></em> y que se le nombra <em><strong>kernel</strong></em>. El <em>kernel</em> inicia con la sintaxis:</p></li>
</ul>
<div class="highlight-C notranslate"><div class="highlight"><pre><span></span><span class="n">__global__</span> <span class="kt">void</span> <span class="n">mifun</span><span class="p">(</span><span class="kt">int</span> <span class="n">param</span><span class="p">){</span>
<span class="p">...</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Siempre es tipo <code class="docutils literal notranslate"><span class="pre">void</span></code> (no hay <code class="docutils literal notranslate"><span class="pre">return</span></code>).</p>
<ul class="simple">
<li><p>El llamado al <em>kernel</em> se realiza desde el <strong>host</strong> y con una sintaxis en la que se define el número de threads, llamados <strong>CUDA threads</strong> (que son distintos a los <em>CPU threads</em>), y bloques, nombrados <strong>CUDA blocks</strong>, que serán utilizados para la ejecución del kernel. La sintaxis que se utiliza es con <code class="docutils literal notranslate"><span class="pre">&lt;&lt;&lt;</span> <span class="pre">&gt;&gt;&gt;</span></code> y en la primera entrada se coloca el número de <em>CUDA blocks</em> y en la segunda entrada el número de <em>CUDA threads</em>:</p></li>
</ul>
<div class="highlight-C notranslate"><div class="highlight"><pre><span></span><span class="n">__global__</span> <span class="kt">void</span> <span class="n">mifun</span><span class="p">(</span><span class="kt">int</span> <span class="n">param</span><span class="p">){</span>
<span class="p">...</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="n">main</span><span class="p">(){</span>
    <span class="kt">int</span> <span class="n">par</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span>
    <span class="n">mifun</span><span class="o">&lt;&lt;&lt;</span><span class="n">N</span><span class="p">,</span><span class="mi">5</span><span class="o">&gt;&gt;&gt;</span> <span class="p">(</span><span class="n">par</span><span class="p">);</span> <span class="c1">//N bloques de 5 threads</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="ejemplo">
<h3>Ejemplo<a class="headerlink" href="#ejemplo" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">hello_world_simple.cu</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%file</span> hello_world_simple.cu
<span class="c1">#include&lt;stdio.h&gt;</span>
<span class="n">__global__</span> <span class="n">void</span> <span class="n">func</span><span class="p">(</span><span class="n">void</span><span class="p">){</span>
<span class="p">}</span>
<span class="nb">int</span> <span class="n">main</span><span class="p">(</span><span class="n">void</span><span class="p">){</span>
    <span class="n">func</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="o">&gt;&gt;&gt;</span><span class="p">();</span>
    <span class="n">printf</span><span class="p">(</span><span class="s2">&quot;Hello world!</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">);</span>
<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Writing hello_world_simple.cu
</pre></div>
</div>
</div>
</div>
<p>Compilación:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
source ~/.profile
nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall hello_world_simple.cu -o hello_world_simple.out
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>nvcc warning : The &#39;compute_35&#39;, &#39;compute_37&#39;, &#39;compute_50&#39;, &#39;sm_35&#39;, &#39;sm_37&#39; and &#39;sm_50&#39; architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
</pre></div>
</div>
</div>
</div>
<p>Ejecución:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
./hello_world_simple.out
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Hello world!
</pre></div>
</div>
</div>
</div>
<div class="admonition-comentarios admonition">
<p class="admonition-title">Comentarios</p>
<ul class="simple">
<li><p>La función <code class="docutils literal notranslate"><span class="pre">main</span></code> se ejecuta en la CPU.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">func</span></code> es un <em>kernel</em> y es ejecutada por los <em>CUDA threads</em> en el <em>device</em>. Obsérvese que tal función inicia con la sintaxis <code class="docutils literal notranslate"><span class="pre">__global__</span></code>. En este caso el <em>CUDA thread</em> que fue lanzado no realiza ninguna acción pues el cuerpo del kernel está vacío.</p></li>
<li><p>El <em>kernel</em> sólo puede tener un <code class="docutils literal notranslate"><span class="pre">return</span></code> tipo <em>void</em>: <code class="docutils literal notranslate"><span class="pre">__global__</span> <span class="pre">void</span> <span class="pre">func</span></code> por lo que el <em>kernel</em> debe regresar sus resultados a través de sus argumentos.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nvcc</span></code> es un <em>wrapper</em> para el compilador de programas escritos en <code class="docutils literal notranslate"><span class="pre">C</span></code>. El compilador instalado en el contenedor de docker descrito al inicio de ésta nota es <code class="docutils literal notranslate"><span class="pre">gcc</span></code>.</p></li>
<li><p>La extensión del archivo debe ser <code class="docutils literal notranslate"><span class="pre">.cu</span></code> aunque esto puede modificarse al compilar con <code class="docutils literal notranslate"><span class="pre">nvcc</span></code>:</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">$nvcc</span> <span class="pre">-x</span> <span class="pre">cu</span> <span class="pre">hello_world.c</span> <span class="pre">-o</span> <span class="pre">hello_world.out</span></code></p>
<ul class="simple">
<li><p>En ocasiones para tener funcionalidad de un determinado <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities">compute capability</a> se especifica la <em>flag</em> de <code class="docutils literal notranslate"><span class="pre">-arch=sm_11</span></code> en la línea de <code class="docutils literal notranslate"><span class="pre">nvcc</span></code>. En este caso se le indica al compilador que compile el programa para un <em>compute capability</em> de <span class="math notranslate nohighlight">\(1.1\)</span>. Ver <a class="reference external" href="https://stackoverflow.com/questions/16954931/cuda-5-0-cudagetdeviceproperties-strange-grid-size-or-a-bug-in-my-code">run a kernel using the larger grid size support offered</a> y <a class="reference external" href="https://stackoverflow.com/questions/35656294/cuda-how-to-use-arch-and-code-and-sm-vs-compute">cuda-how-to-use-arch-and-code-and-sm-vs-compute</a> para más sobre esto.</p></li>
</ul>
</div>
</div>
<div class="section" id="bloques-de-threads">
<h3>¿Bloques de threads?<a class="headerlink" href="#bloques-de-threads" title="Permalink to this headline">¶</a></h3>
<p>Los <em>CUDA threads</em> son divididos en <em>CUDA blocks</em> y éstos se encuentran en un <em>grid</em>. En el lanzamiento del <em>kernel</em> se debe especificar al hardware cuántos <em>CUDA blocks</em> tendrá nuestro <em>grid</em> y cuántos <em>CUDA threads</em> estarán en cada bloque.</p>
</div>
<div class="section" id="id1">
<h3>Ejemplo<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%file</span> hello_world_2.cu
<span class="c1">#include&lt;stdio.h&gt;</span>
<span class="n">__global__</span> <span class="n">void</span> <span class="n">func</span><span class="p">(</span><span class="n">void</span><span class="p">){</span>
    <span class="n">printf</span><span class="p">(</span><span class="s2">&quot;Hello world! del bloque </span><span class="si">%d</span><span class="s2"> del thread </span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">blockIdx</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span><span class="p">);</span>
<span class="p">}</span>
<span class="nb">int</span> <span class="n">main</span><span class="p">(</span><span class="n">void</span><span class="p">){</span>
    <span class="n">func</span><span class="o">&lt;&lt;&lt;</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="o">&gt;&gt;&gt;</span><span class="p">();</span> <span class="o">//</span><span class="mi">2</span> <span class="n">bloques</span> <span class="n">de</span> <span class="mi">3</span> <span class="n">threads</span> <span class="n">cada</span> <span class="n">uno</span>
    <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
    <span class="o">//</span><span class="n">printf</span><span class="p">(</span><span class="s2">&quot;Hello world! del cpu thread</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">);</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Writing hello_world_2.cu
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
source ~/.profile
nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall hello_world_2.cu -o hello_world_2.out
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>nvcc warning : The &#39;compute_35&#39;, &#39;compute_37&#39;, &#39;compute_50&#39;, &#39;sm_35&#39;, &#39;sm_37&#39; and &#39;sm_50&#39; architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
./hello_world_2.out
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Hello world! del bloque 0 del thread 0
Hello world! del bloque 0 del thread 1
Hello world! del bloque 0 del thread 2
Hello world! del bloque 1 del thread 0
Hello world! del bloque 1 del thread 1
Hello world! del bloque 1 del thread 2
</pre></div>
</div>
</div>
</div>
<div class="admonition-comentarios admonition">
<p class="admonition-title">Comentarios</p>
<ul class="simple">
<li><p>En lo que continúa de la nota el nombre <em>thread</em> hará referencia a <em>CUDA thread</em> y el nombre bloque a <em>CUDA block</em>.</p></li>
<li><p>El llamado a la ejecución del kernel se realizó en el <em>host</em> y se lanzaron <span class="math notranslate nohighlight">\(2\)</span> bloques (primera posición en la sintaxis &lt;&lt;&lt;&gt;&gt;&gt;), cada uno con <span class="math notranslate nohighlight">\(3\)</span> <em>threads</em>.</p></li>
<li><p>Se utiliza la función <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g10e20b05a95f638a4071a655503df25d">cudaDeviceSynchronize</a> para que el <em>cpu-thread</em> espere la finalización de la ejecución del kernel.</p></li>
<li><p>En el ejemplo anterior, las variables <code class="docutils literal notranslate"><span class="pre">blockIdx</span></code> y <code class="docutils literal notranslate"><span class="pre">threadIdx</span></code> hacen referencia a los <strong>id</strong>’s que tienen los bloques y los threads: el <em>id</em> del bloque dentro del <em>grid</em> y el <em>id</em> del thread dentro del bloque. La parte <code class="docutils literal notranslate"><span class="pre">.x</span></code> de las variables: <code class="docutils literal notranslate"><span class="pre">blockIdx.x</span></code> y <code class="docutils literal notranslate"><span class="pre">threadIdx.x</span></code> refieren a la <strong>primera coordenada</strong> del bloque en el <em>grid</em> y a la <strong>primera coordenada</strong> del <em>thread</em> en en el bloque.</p></li>
<li><p>La elección del número de bloques en un grid o el número de <em>threads</em> en un bloque no corresponde a alguna disposición del hardware, esto es, si se lanza un kernel con <code class="docutils literal notranslate"><span class="pre">&lt;&lt;&lt;</span> <span class="pre">1,</span> <span class="pre">3</span> <span class="pre">&gt;&gt;&gt;</span></code> no implica que la GPU tenga en su hardware un bloque o 3 <em>threads</em>. Asimismo, las coordenadas que se obtienen vía <code class="docutils literal notranslate"><span class="pre">blockIdx</span></code> o <code class="docutils literal notranslate"><span class="pre">threadIdx</span></code> son meras abstracciones, no corresponden a algún ordenamiento en el hardware de la GPU.</p></li>
<li><p>Todos los <em>threads</em> de un bloque  ejecutan el kernel por lo que se tienen tantas copias del kernel como número de bloques sean lanzados. Esto es una muestra la GPU sigue el modelo  <em>Single Instruction Multiple Threads <a class="reference external" href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads">(SIMT)</a></em>.</p></li>
</ul>
</div>
</div>
<div class="section" id="grid-s-y-bloques-3-dimensionales">
<h3>¿Grid’s y bloques 3-dimensionales?<a class="headerlink" href="#grid-s-y-bloques-3-dimensionales" title="Permalink to this headline">¶</a></h3>
<p>En el <em>device</em> podemos definir el <em>grid</em> de bloques y el bloque de <em>threads</em> utilizando el tipo de dato <code class="docutils literal notranslate"><span class="pre">dim3</span></code> el cual también es parte de <em>CUDA C</em>:</p>
</div>
<div class="section" id="id2">
<h3>Ejemplo<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%file</span> hello_world_3.cu
<span class="c1">#include&lt;stdio.h&gt;</span>
<span class="n">__global__</span> <span class="n">void</span> <span class="n">func</span><span class="p">(</span><span class="n">void</span><span class="p">){</span>
    <span class="n">printf</span><span class="p">(</span><span class="s2">&quot;Hello world! del bloque </span><span class="si">%d</span><span class="s2"> del thread </span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">blockIdx</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">threadIdx</span><span class="o">.</span><span class="n">z</span><span class="p">);</span>
<span class="p">}</span>
<span class="nb">int</span> <span class="n">main</span><span class="p">(</span><span class="n">void</span><span class="p">){</span>
    <span class="n">dim3</span> <span class="n">dimGrid</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span> <span class="o">//</span><span class="mi">2</span> <span class="n">bloques</span> <span class="n">en</span> <span class="n">el</span> <span class="n">grid</span>
    <span class="n">dim3</span> <span class="n">dimBlock</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">);</span> <span class="o">//</span><span class="mi">3</span> <span class="n">threads</span> <span class="n">por</span> <span class="n">bloque</span>
    <span class="n">func</span><span class="o">&lt;&lt;&lt;</span><span class="n">dimGrid</span><span class="p">,</span><span class="n">dimBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">();</span> 
    <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
    <span class="n">printf</span><span class="p">(</span><span class="s2">&quot;Hello world! del cpu thread</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">);</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Writing hello_world_3.cu
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
source ~/.profile
nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall hello_world_3.cu -o hello_world_3.out
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>nvcc warning : The &#39;compute_35&#39;, &#39;compute_37&#39;, &#39;compute_50&#39;, &#39;sm_35&#39;, &#39;sm_37&#39; and &#39;sm_50&#39; architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
./hello_world_3.out
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Hello world! del bloque 0 del thread 0
Hello world! del bloque 0 del thread 1
Hello world! del bloque 0 del thread 2
Hello world! del bloque 1 del thread 0
Hello world! del bloque 1 del thread 1
Hello world! del bloque 1 del thread 2
Hello world! del cpu thread
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id3">
<h3>Ejemplo<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%file</span> thread_idxs.cu
<span class="c1">#include&lt;stdio.h&gt;</span>
<span class="n">__global__</span> <span class="n">void</span> <span class="n">func</span><span class="p">(</span><span class="n">void</span><span class="p">){</span>
    <span class="k">if</span><span class="p">(</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span><span class="o">==</span><span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">threadIdx</span><span class="o">.</span><span class="n">y</span><span class="o">==</span><span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">threadIdx</span><span class="o">.</span><span class="n">z</span><span class="o">==</span><span class="mi">0</span><span class="p">){</span>
        <span class="n">printf</span><span class="p">(</span><span class="s2">&quot;blockIdx.x:</span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">x</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">printf</span><span class="p">(</span><span class="s2">&quot;thread idx.x:</span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s2">&quot;thread idx.y:</span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">y</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s2">&quot;thread idx.z:</span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">z</span><span class="p">);</span>
<span class="p">}</span>
<span class="nb">int</span> <span class="n">main</span><span class="p">(</span><span class="n">void</span><span class="p">){</span>
    <span class="n">dim3</span> <span class="n">dimGrid</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span> <span class="o">//</span><span class="mi">1</span> <span class="n">bloque</span> <span class="n">en</span> <span class="n">el</span> <span class="n">grid</span>
    <span class="n">dim3</span> <span class="n">dimBlock</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span> <span class="o">//</span><span class="mi">3</span> <span class="n">threads</span> <span class="n">por</span> <span class="n">bloque</span>
    <span class="n">func</span><span class="o">&lt;&lt;&lt;</span><span class="n">dimGrid</span><span class="p">,</span><span class="n">dimBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">();</span> 
    <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Writing thread_idxs.cu
</pre></div>
</div>
</div>
</div>
<div class="admonition-comentarios admonition">
<p class="admonition-title">Comentarios</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dim3</span> <span class="pre">dimGrid(1,1,1);</span></code> representa 1 bloque en el <em>grid</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dim3</span> <span class="pre">dimBlock(1,3,1);</span></code> representa 3 <em>threads</em> por bloque.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span> 
source ~/.profile
nvcc -gencode arch=compute_37,code=sm_37 thread_idxs.cu -o thread_idxs.out 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>nvcc warning : The &#39;compute_35&#39;, &#39;compute_37&#39;, &#39;compute_50&#39;, &#39;sm_35&#39;, &#39;sm_37&#39; and &#39;sm_50&#39; architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
./thread_idxs.out
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>blockIdx.x:0
thread idx.x:0
thread idx.x:0
thread idx.x:0
thread idx.y:0
thread idx.y:1
thread idx.y:2
thread idx.z:0
thread idx.z:0
thread idx.z:0
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id4">
<h3>Ejemplo<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%file</span> block_idxs.cu
<span class="c1">#include&lt;stdio.h&gt;</span>
<span class="n">__global__</span> <span class="n">void</span> <span class="n">func</span><span class="p">(</span><span class="n">void</span><span class="p">){</span>
    <span class="n">printf</span><span class="p">(</span><span class="s2">&quot;blockIdx.x:</span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">x</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s2">&quot;blockIdx.y:</span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">y</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s2">&quot;blockIdx.z:</span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">z</span><span class="p">);</span>

<span class="p">}</span>
<span class="nb">int</span> <span class="n">main</span><span class="p">(</span><span class="n">void</span><span class="p">){</span>
    <span class="n">dim3</span> <span class="n">dimGrid</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">);</span> 
    <span class="n">dim3</span> <span class="n">dimBlock</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span> 
    <span class="n">func</span><span class="o">&lt;&lt;&lt;</span><span class="n">dimGrid</span><span class="p">,</span><span class="n">dimBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">();</span> 
    <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Writing block_idxs.cu
</pre></div>
</div>
</div>
</div>
<div class="admonition-comentarios admonition">
<p class="admonition-title">Comentarios</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dim3</span> <span class="pre">dimGrid(1,2,2);</span></code> representa 4 bloques en el <em>grid</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dim3</span> <span class="pre">dimBlock(1,1,1);</span></code> representa 1 <em>thread</em> por bloque.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
source ~/.profile
nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall block_idxs.cu -o block_idxs.out
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>nvcc warning : The &#39;compute_35&#39;, &#39;compute_37&#39;, &#39;compute_50&#39;, &#39;sm_35&#39;, &#39;sm_37&#39; and &#39;sm_50&#39; architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
./block_idxs.out
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>blockIdx.x:0
blockIdx.x:0
blockIdx.x:0
blockIdx.x:0
blockIdx.y:1
blockIdx.y:0
blockIdx.y:1
blockIdx.y:0
blockIdx.z:1
blockIdx.z:0
blockIdx.z:0
blockIdx.z:1
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id5">
<h3>Ejemplo<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>Podemos usar la variable <code class="docutils literal notranslate"><span class="pre">blockDim</span></code> para cada coordenada <code class="docutils literal notranslate"><span class="pre">x,</span> <span class="pre">y</span></code> o <code class="docutils literal notranslate"><span class="pre">z</span></code> y obtener la dimensión de los bloques:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%file</span> block_dims.cu
<span class="c1">#include&lt;stdio.h&gt;</span>
<span class="n">__global__</span> <span class="n">void</span> <span class="n">func</span><span class="p">(</span><span class="n">void</span><span class="p">){</span>
    <span class="k">if</span><span class="p">(</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span><span class="o">==</span><span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">threadIdx</span><span class="o">.</span><span class="n">y</span><span class="o">==</span><span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">threadIdx</span><span class="o">.</span><span class="n">z</span><span class="o">==</span><span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">blockIdx</span><span class="o">.</span><span class="n">z</span><span class="o">==</span><span class="mi">1</span><span class="p">){</span>
    <span class="n">printf</span><span class="p">(</span><span class="s2">&quot;blockDim.x:</span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">blockDim</span><span class="o">.</span><span class="n">x</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s2">&quot;blockDim.y:</span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">blockDim</span><span class="o">.</span><span class="n">y</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s2">&quot;blockDim.z:</span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">blockDim</span><span class="o">.</span><span class="n">z</span><span class="p">);</span>
    <span class="p">}</span>

<span class="p">}</span>
<span class="nb">int</span> <span class="n">main</span><span class="p">(</span><span class="n">void</span><span class="p">){</span>
    <span class="n">dim3</span> <span class="n">dimGrid</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">);</span>
    <span class="n">dim3</span> <span class="n">dimBlock</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">);</span>
    <span class="n">func</span><span class="o">&lt;&lt;&lt;</span><span class="n">dimGrid</span><span class="p">,</span><span class="n">dimBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">();</span> 
    <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Writing block_dims.cu
</pre></div>
</div>
</div>
</div>
<div class="admonition-comentarios admonition">
<p class="admonition-title">Comentarios</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dim3</span> <span class="pre">dimGrid(2,2,2);</span></code> representa 8 bloques en el <em>grid</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dim3</span> <span class="pre">dimBlock(3,1,2);</span></code> representa 6 <em>threads</em> por bloque.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
source ~/.profile
nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall block_dims.cu -o block_dims.out
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>nvcc warning : The &#39;compute_35&#39;, &#39;compute_37&#39;, &#39;compute_50&#39;, &#39;sm_35&#39;, &#39;sm_37&#39; and &#39;sm_50&#39; architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
./block_dims.out
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>blockDim.x:3
blockDim.x:3
blockDim.x:3
blockDim.x:3
blockDim.y:1
blockDim.y:1
blockDim.y:1
blockDim.y:1
blockDim.z:2
blockDim.z:2
blockDim.z:2
blockDim.z:2
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="alojamiento-de-memoria-en-el-device">
<h3>Alojamiento de memoria en el <em>device</em><a class="headerlink" href="#alojamiento-de-memoria-en-el-device" title="Permalink to this headline">¶</a></h3>
<p>Para alojar memoria en el <em>device</em> se utiliza el llamado a <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g37d37965bfb4803b6d4e59ff26856356">cudaMalloc</a> y para transferir datos del <em>host</em> al <em>device</em> o viceversa se llama a lafunción <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1gc263dbe6574220cc776b45438fc351e8">cudaMemcpy</a> con respectivos parámetros como <code class="docutils literal notranslate"><span class="pre">cudaMemcpyHostToDevice</span></code> o <code class="docutils literal notranslate"><span class="pre">cudaMemcpyDeviceToHost</span></code>.</p>
<p>Para desalojar memoria del <em>device</em> se utiliza el llamado a <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ga042655cbbf3408f01061652a075e094">cudaFree</a>.</p>
</div>
<div class="section" id="id6">
<h3>Ejemplo<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p><strong>N bloques de 1 thread</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%file</span> vector_sum.cu
<span class="c1">#include&lt;stdio.h&gt;</span>
<span class="c1">#define N 10</span>
<span class="n">__global__</span> <span class="n">void</span> <span class="n">vect_sum</span><span class="p">(</span><span class="nb">int</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="nb">int</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="nb">int</span> <span class="o">*</span><span class="n">c</span><span class="p">){</span>
    <span class="nb">int</span> <span class="n">block_id_x</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="o">.</span><span class="n">x</span><span class="p">;</span>
    <span class="k">if</span><span class="p">(</span><span class="n">block_id_x</span><span class="o">&lt;</span><span class="n">N</span><span class="p">)</span> <span class="o">//</span><span class="n">we</span> <span class="n">assume</span> <span class="n">N</span> <span class="ow">is</span> <span class="n">less</span> <span class="n">than</span> <span class="n">maximum</span> <span class="n">number</span> <span class="n">of</span> <span class="n">blocks</span>
                     <span class="o">//</span><span class="n">that</span> <span class="n">can</span> <span class="n">be</span> <span class="n">launched</span>
        <span class="n">c</span><span class="p">[</span><span class="n">block_id_x</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">block_id_x</span><span class="p">]</span><span class="o">+</span><span class="n">b</span><span class="p">[</span><span class="n">block_id_x</span><span class="p">];</span>
<span class="p">}</span>
<span class="nb">int</span> <span class="n">main</span><span class="p">(</span><span class="n">void</span><span class="p">){</span>
    <span class="nb">int</span> <span class="n">a</span><span class="p">[</span><span class="n">N</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">N</span><span class="p">],</span><span class="n">c</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
    <span class="nb">int</span> <span class="o">*</span><span class="n">device_a</span><span class="p">,</span> <span class="o">*</span><span class="n">device_b</span><span class="p">,</span> <span class="o">*</span><span class="n">device_c</span><span class="p">;</span>
    <span class="nb">int</span> <span class="n">i</span><span class="p">;</span>
    <span class="n">dim3</span> <span class="n">dimGrid</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
    <span class="n">dim3</span> <span class="n">dimBlock</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
    <span class="o">//</span><span class="n">allocation</span> <span class="ow">in</span> <span class="n">device</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="n">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">device_a</span><span class="p">,</span> <span class="n">sizeof</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">*</span><span class="n">N</span><span class="p">);</span> 
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="n">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">device_b</span><span class="p">,</span> <span class="n">sizeof</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">*</span><span class="n">N</span><span class="p">);</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="n">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">device_c</span><span class="p">,</span> <span class="n">sizeof</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">*</span><span class="n">N</span><span class="p">);</span>
    <span class="o">//</span><span class="n">dummy</span> <span class="n">data</span>
    <span class="k">for</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">N</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">){</span>
        <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">i</span><span class="p">;</span>
        <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">i</span><span class="o">*</span><span class="n">i</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="o">//</span><span class="n">making</span> <span class="n">copies</span> <span class="n">of</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="n">arrays</span> <span class="n">to</span> <span class="n">GPU</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">device_a</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">N</span><span class="o">*</span><span class="n">sizeof</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">device_b</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">N</span><span class="o">*</span><span class="n">sizeof</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
    <span class="o">//</span><span class="n">mandamos</span> <span class="n">a</span> <span class="n">llamar</span> <span class="n">a</span> <span class="n">suma_vect</span><span class="p">:</span>
    <span class="n">vect_sum</span><span class="o">&lt;&lt;&lt;</span><span class="n">dimGrid</span><span class="p">,</span><span class="n">dimBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">device_a</span><span class="p">,</span><span class="n">device_b</span><span class="p">,</span><span class="n">device_c</span><span class="p">);</span>
    <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
    <span class="o">//</span><span class="n">copy</span> <span class="n">result</span> <span class="n">to</span> <span class="n">c</span> <span class="n">array</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="n">device_c</span><span class="p">,</span><span class="n">N</span><span class="o">*</span><span class="n">sizeof</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span><span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
    <span class="k">for</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">N</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span>
        <span class="n">printf</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%d</span><span class="s2">+</span><span class="si">%d</span><span class="s2"> = </span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">device_a</span><span class="p">);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">device_b</span><span class="p">);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">device_c</span><span class="p">);</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Writing vector_sum.cu
</pre></div>
</div>
</div>
</div>
<div class="admonition-comentarios admonition">
<p class="admonition-title">Comentarios</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dim3</span> <span class="pre">dimGrid(N,1,1);</span></code> representa N bloques en el <em>grid</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dim3</span> <span class="pre">dimBlock(1,1,1);</span></code> representa 1 <em>thread</em> por bloque.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;&lt;&lt;dimGrid,dimBlock&gt;&gt;&gt;</span></code> N bloques de 1 <em>thread</em>.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
source ~/.profile
nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall vector_sum.cu -o vector_sum.out
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>nvcc warning : The &#39;compute_35&#39;, &#39;compute_37&#39;, &#39;compute_50&#39;, &#39;sm_35&#39;, &#39;sm_37&#39; and &#39;sm_50&#39; architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
./vector_sum.out
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0+0 = 0
1+1 = 2
2+4 = 6
3+9 = 12
4+16 = 20
5+25 = 30
6+36 = 42
7+49 = 56
8+64 = 72
9+81 = 90
</pre></div>
</div>
</div>
</div>
<div class="admonition-comentarios admonition">
<p class="admonition-title">Comentarios</p>
<ul class="simple">
<li><p>Los <em>statements</em>:</p></li>
</ul>
<div class="highlight-C notranslate"><div class="highlight"><pre><span></span>    <span class="kt">int</span> <span class="o">*</span><span class="n">device_a</span><span class="p">,</span> <span class="o">*</span><span class="n">device_b</span><span class="p">,</span> <span class="o">*</span><span class="n">device_c</span><span class="p">;</span>
</pre></div>
</div>
<p>en sintaxis de <em>C</em> se definen apuntadores que refieren a una dirección de memoria. En el contexto de la <em>GPU programming</em> estos apuntadores no apuntan a una dirección de memoria en el <em>device</em>. Aunque NVIDIA añadió el <em>feature</em> de <a class="reference external" href="https://devblogs.nvidia.com/unified-memory-cuda-beginners/">Unified Memory</a> (un espacio de memoria accesible para el <em>host</em> y el <em>device</em>) aquí no se está usando tal <em>feature</em>. Más bien se están utilizando los apuntadores anteriores para apuntar a un <a class="reference external" href="https://en.wikipedia.org/wiki/Struct_(C_programming_language)">struct</a> de <em>C</em> en el que uno de sus tipos de datos es una dirección de memoria en el <em>device</em>.</p>
<ul class="simple">
<li><p>El uso de <code class="docutils literal notranslate"><span class="pre">(void</span> <span class="pre">**)</span></code> es por la definición de la función <code class="docutils literal notranslate"><span class="pre">cudaMalloc</span></code>.</p></li>
<li><p>En el programa anterior se coloca en comentario que se asume que <span class="math notranslate nohighlight">\(N\)</span> el número de datos en el arreglo es menor al número de bloques que es posible lanzar. Esto como veremos más adelante es importante considerar pues aunque en un <em>device</em> se pueden lanzar muchos bloques y muchos threads, se tienen límites en el número de éstos que es posible lanzar.</p></li>
</ul>
</div>
</div>
<div class="section" id="perfilamiento-en-cuda">
<h3>¿Perfilamiento en CUDA?<a class="headerlink" href="#perfilamiento-en-cuda" title="Permalink to this headline">¶</a></h3>
<p>Al instalar el <em>CUDA toolkit</em> en sus máquinas o bien si utilizan el contenedor de docker (descrito al inicio de la nota) se instala la línea de comando <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html">nvprof</a> para perfilamiento. Se puede ejecutar con:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
source ~/.profile
nvprof --normalized-time-unit s ./vector_sum.out
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0+0 = 0
1+1 = 2
2+4 = 6
3+9 = 12
4+16 = 20
5+25 = 30
6+36 = 42
7+49 = 56
8+64 = 72
9+81 = 90
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>==6052== NVPROF is profiling process 6052, command: ./vector_sum.out
==6052== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.
==6052== Profiling application: ./vector_sum.out
==6052== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
                        %         s                   s         s         s
 GPU activities:    44.81  5.25e-06         1  5.25e-06  5.25e-06  5.25e-06  vect_sum(int*, int*, int*)
                    30.87  3.62e-06         2  1.81e-06  1.54e-06  2.08e-06  [CUDA memcpy HtoD]
                    24.32  2.85e-06         1  2.85e-06  2.85e-06  2.85e-06  [CUDA memcpy DtoH]
      API calls:    99.60  0.269994         3  0.089998  3.85e-06  0.269984  cudaMalloc
                     0.19  5.14e-04         1  5.14e-04  5.14e-04  5.14e-04  cuDeviceTotalMem
                     0.10  2.74e-04       101  2.71e-06  7.41e-07  9.10e-05  cuDeviceGetAttribute
                     0.06  1.56e-04         3  5.19e-05  4.56e-06  1.42e-04  cudaFree
                     0.02  5.80e-05         3  1.93e-05  1.01e-05  2.63e-05  cudaMemcpy
                     0.01  4.06e-05         1  4.06e-05  4.06e-05  4.06e-05  cudaLaunchKernel
                     0.01  2.40e-05         1  2.40e-05  2.40e-05  2.40e-05  cuDeviceGetName
                     0.00  1.12e-05         1  1.12e-05  1.12e-05  1.12e-05  cudaDeviceSynchronize
                     0.00  7.54e-06         1  7.54e-06  7.54e-06  7.54e-06  cuDeviceGetPCIBusId
                     0.00  3.78e-06         3  1.26e-06  7.60e-07  1.86e-06  cuDeviceGetCount
                     0.00  2.88e-06         2  1.44e-06  7.94e-07  2.09e-06  cuDeviceGet
                     0.00  9.18e-07         1  9.18e-07  9.18e-07  9.18e-07  cuDeviceGetUuid
</pre></div>
</div>
</div>
</div>
<p><strong>Comentarios:</strong></p>
<ul class="simple">
<li><p>Las unidades en las que se reporta son s: second, ms: millisecond, us: microsecond, ns: nanosecond.</p></li>
<li><p>En la documentación de NVIDIA se menciona que <code class="docutils literal notranslate"><span class="pre">nvprof</span></code> será reemplazada próximamente por <a class="reference external" href="https://developer.nvidia.com/nsight-compute">NVIDIA Nsight Compute</a> y <a class="reference external" href="https://developer.nvidia.com/nsight-systems">NVIDIA Nsight Systems</a>.</p></li>
</ul>
<p>En el ejemplo anterior se lanzaron <span class="math notranslate nohighlight">\(N\)</span> bloques con <span class="math notranslate nohighlight">\(1\)</span> <em>thread</em> cada uno y a continuación se lanza <span class="math notranslate nohighlight">\(1\)</span> bloque con <span class="math notranslate nohighlight">\(N\)</span> <em>threads</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%file</span> vector_sum_2.cu
<span class="c1">#include&lt;stdio.h&gt;</span>
<span class="c1">#define N 10</span>
<span class="n">__global__</span> <span class="n">void</span> <span class="n">vect_sum</span><span class="p">(</span><span class="nb">int</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="nb">int</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="nb">int</span> <span class="o">*</span><span class="n">c</span><span class="p">){</span>
    <span class="nb">int</span> <span class="n">thread_id_x</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span><span class="p">;</span>
    <span class="k">if</span><span class="p">(</span><span class="n">thread_id_x</span><span class="o">&lt;</span><span class="n">N</span><span class="p">)</span> 
        <span class="n">c</span><span class="p">[</span><span class="n">thread_id_x</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">thread_id_x</span><span class="p">]</span><span class="o">+</span><span class="n">b</span><span class="p">[</span><span class="n">thread_id_x</span><span class="p">];</span>
<span class="p">}</span>
<span class="nb">int</span> <span class="n">main</span><span class="p">(</span><span class="n">void</span><span class="p">){</span>
    <span class="nb">int</span> <span class="o">*</span><span class="n">device_a</span><span class="p">,</span> <span class="o">*</span><span class="n">device_b</span><span class="p">,</span> <span class="o">*</span><span class="n">device_c</span><span class="p">;</span>
    <span class="nb">int</span> <span class="n">i</span><span class="p">;</span>
    <span class="n">dim3</span> <span class="n">dimGrid</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
    <span class="n">dim3</span> <span class="n">dimBlock</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
    <span class="o">//</span><span class="n">alojando</span> <span class="n">en</span> <span class="n">device</span> <span class="n">con</span> <span class="n">Unified</span> <span class="n">Memory</span>
    <span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">device_a</span><span class="p">,</span> <span class="n">sizeof</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">*</span><span class="n">N</span><span class="p">);</span>
    <span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">device_b</span><span class="p">,</span> <span class="n">sizeof</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">*</span><span class="n">N</span><span class="p">);</span>
    <span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">device_c</span><span class="p">,</span> <span class="n">sizeof</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">*</span><span class="n">N</span><span class="p">);</span>
    <span class="o">//</span><span class="n">llenando</span> <span class="n">los</span> <span class="n">arreglos</span><span class="p">:</span>
    <span class="k">for</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">N</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">){</span>
        <span class="n">device_a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">i</span><span class="p">;</span>
        <span class="n">device_b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">i</span><span class="o">*</span><span class="n">i</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">vect_sum</span><span class="o">&lt;&lt;&lt;</span><span class="n">dimGrid</span><span class="p">,</span><span class="n">dimBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">device_a</span><span class="p">,</span><span class="n">device_b</span><span class="p">,</span><span class="n">device_c</span><span class="p">);</span> 
    <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
    <span class="k">for</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">N</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span>
        <span class="n">printf</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%d</span><span class="s2">+</span><span class="si">%d</span><span class="s2"> = </span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">device_a</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">device_b</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">device_c</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">device_a</span><span class="p">);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">device_b</span><span class="p">);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">device_c</span><span class="p">);</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Writing vector_sum_2.cu
</pre></div>
</div>
</div>
</div>
<div class="admonition-comentarios admonition">
<p class="admonition-title">Comentarios</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dim3</span> <span class="pre">dimGrid(1,1,1);</span></code> representa 1 bloque en el <em>grid</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dim3</span> <span class="pre">dimBlock(N,1,1);</span></code> representa N <em>threads</em> por bloque.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;&lt;&lt;dimGrid,dimBlock&gt;&gt;&gt;</span></code> 1 bloque con N <em>threads</em>.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
source ~/.profile
nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall vector_sum_2.cu -o vector_sum_2.out
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>nvcc warning : The &#39;compute_35&#39;, &#39;compute_37&#39;, &#39;compute_50&#39;, &#39;sm_35&#39;, &#39;sm_37&#39; and &#39;sm_50&#39; architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
source ~/.profile
nvprof --normalized-time-unit s ./vector_sum_2.out
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0+0 = 0
1+1 = 2
2+4 = 6
3+9 = 12
4+16 = 20
5+25 = 30
6+36 = 42
7+49 = 56
8+64 = 72
9+81 = 90
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>==6101== NVPROF is profiling process 6101, command: ./vector_sum_2.out
==6101== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.
==6101== Profiling application: ./vector_sum_2.out
==6101== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
                        %         s                   s         s         s
 GPU activities:   100.00  5.79e-06         1  5.79e-06  5.79e-06  5.79e-06  vect_sum(int*, int*, int*)
      API calls:    99.58  0.303429         3  0.101143  7.14e-06  0.303394  cudaMallocManaged
                     0.17  5.21e-04         1  5.21e-04  5.21e-04  5.21e-04  cuDeviceTotalMem
                     0.09  2.86e-04       101  2.83e-06  7.94e-07  9.55e-05  cuDeviceGetAttribute
                     0.08  2.45e-04         1  2.45e-04  2.45e-04  2.45e-04  cudaLaunchKernel
                     0.05  1.62e-04         3  5.41e-05  9.79e-06  1.26e-04  cudaFree
                     0.01  3.50e-05         1  3.50e-05  3.50e-05  3.50e-05  cuDeviceGetName
                     0.01  2.09e-05         1  2.09e-05  2.09e-05  2.09e-05  cudaDeviceSynchronize
                     0.00  9.29e-06         1  9.29e-06  9.29e-06  9.29e-06  cuDeviceGetPCIBusId
                     0.00  3.90e-06         3  1.30e-06  8.18e-07  1.86e-06  cuDeviceGetCount
                     0.00  2.55e-06         2  1.27e-06  8.67e-07  1.68e-06  cuDeviceGet
                     0.00  9.52e-07         1  9.52e-07  9.52e-07  9.52e-07  cuDeviceGetUuid

==6101== Unified Memory profiling result:
Device &quot;NVIDIA Tesla K80 (0)&quot;
   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name
       1  8.0000KB  8.0000KB  8.0000KB  8.000000KB  4.2880e-06s  Host To Device
       5  25.600KB  4.0000KB  60.000KB  128.0000KB  2.7362e-05s  Device To Host
Total CPU Page faults: 2
</pre></div>
</div>
</div>
</div>
<p><strong>Comentarios:</strong></p>
<ul class="simple">
<li><p>El programa anterior utiliza la <a class="reference external" href="https://devblogs.nvidia.com/unified-memory-cuda-beginners/">Unified Memory</a> con la función <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gcf6b9b1019e73c5bc2b39b39fe90816e">cudaMallocManaged</a>. La <em>Unified Memory</em> es un <em>feature</em> que se añadió a CUDA desde las arquitecturas de <strong>Kepler</strong> y <strong>Maxwell</strong> pero que ha ido mejorando (por ejemplo añadiendo <a class="reference external" href="https://en.wikipedia.org/wiki/Page_fault">page faulting</a> and <a class="reference external" href="https://www.kernel.org/doc/html/latest/vm/page_migration.html">migration</a>) en las arquitecturas siguientes a la de <em>Kepler</em>: la arquitectura Pascal y Volta. Por esto en el <em>output</em> anterior de <em>nvprof</em> aparece una sección de <em>page fault</em>.</p></li>
<li><p>Obsérvese que en el programa anterior se comenta que se asume que <span class="math notranslate nohighlight">\(N\)</span> el número de datos en el arreglo es menor al número de <em>threads</em> que es posible lanzar. Esto como veremos más adelante es importante considerar pues aunque en el <em>device</em> se pueden lanzar muchos bloques y muchos threads, se tienen límites en el número de éstos que es posible lanzar.</p></li>
</ul>
</div>
<div class="section" id="tenemos-que-inicializar-los-datos-en-la-cpu-y-copiarlos-hacia-la-gpu">
<h3>¿Tenemos que inicializar los datos en la CPU y copiarlos hacia la GPU?<a class="headerlink" href="#tenemos-que-inicializar-los-datos-en-la-cpu-y-copiarlos-hacia-la-gpu" title="Permalink to this headline">¶</a></h3>
<p>En realidad no tenemos que realizarlo para el ejemplo de <code class="docutils literal notranslate"><span class="pre">suma_vectorial.cu</span></code> anterior. Por ejemplo:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%file</span> vector_sum_3.cu
<span class="c1">#include&lt;stdio.h&gt;</span>
<span class="c1">#define N 10</span>
<span class="n">__global__</span> <span class="n">void</span> <span class="n">fill_arrays</span><span class="p">(</span><span class="nb">int</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="nb">int</span> <span class="o">*</span><span class="n">b</span><span class="p">){</span>
    <span class="nb">int</span> <span class="n">thread_id_x</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span><span class="p">;</span>
    <span class="n">a</span><span class="p">[</span><span class="n">thread_id_x</span><span class="p">]</span><span class="o">=</span><span class="n">thread_id_x</span><span class="p">;</span>
    <span class="n">b</span><span class="p">[</span><span class="n">thread_id_x</span><span class="p">]</span><span class="o">=</span><span class="n">thread_id_x</span><span class="o">*</span><span class="n">thread_id_x</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">__global__</span> <span class="n">void</span> <span class="n">vect_sum</span><span class="p">(</span><span class="nb">int</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="nb">int</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="nb">int</span> <span class="o">*</span><span class="n">c</span><span class="p">){</span>
    <span class="nb">int</span> <span class="n">thread_id_x</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span><span class="p">;</span>
    <span class="k">if</span><span class="p">(</span><span class="n">thread_id_x</span><span class="o">&lt;</span><span class="n">N</span><span class="p">)</span>
        <span class="n">c</span><span class="p">[</span><span class="n">thread_id_x</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">thread_id_x</span><span class="p">]</span><span class="o">+</span><span class="n">b</span><span class="p">[</span><span class="n">thread_id_x</span><span class="p">];</span>
<span class="p">}</span>

<span class="nb">int</span> <span class="n">main</span><span class="p">(</span><span class="n">void</span><span class="p">){</span>
    <span class="nb">int</span> <span class="o">*</span><span class="n">device_a</span><span class="p">,</span> <span class="o">*</span><span class="n">device_b</span><span class="p">,</span> <span class="o">*</span><span class="n">device_c</span><span class="p">;</span>
    <span class="nb">int</span> <span class="n">i</span><span class="p">;</span>
    <span class="n">dim3</span> <span class="n">dimGrid</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
    <span class="n">dim3</span> <span class="n">dimBlock</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
    <span class="o">//</span><span class="n">allocating</span> <span class="n">using</span> <span class="n">Unified</span> <span class="n">Memory</span> <span class="ow">in</span> <span class="n">device</span>
    <span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">device_a</span><span class="p">,</span> <span class="n">sizeof</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">*</span><span class="n">N</span><span class="p">);</span>
    <span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">device_b</span><span class="p">,</span> <span class="n">sizeof</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">*</span><span class="n">N</span><span class="p">);</span>
    <span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">device_c</span><span class="p">,</span> <span class="n">sizeof</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">*</span><span class="n">N</span><span class="p">);</span>
    <span class="n">fill_arrays</span><span class="o">&lt;&lt;&lt;</span><span class="n">dimGrid</span><span class="p">,</span><span class="n">dimBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">device_a</span><span class="p">,</span><span class="n">device_b</span><span class="p">);</span>
    <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
    <span class="n">vect_sum</span><span class="o">&lt;&lt;&lt;</span><span class="n">dimGrid</span><span class="p">,</span><span class="n">dimBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">device_a</span><span class="p">,</span><span class="n">device_b</span><span class="p">,</span><span class="n">device_c</span><span class="p">);</span>
    <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
    <span class="k">for</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">N</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span>
        <span class="n">printf</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%d</span><span class="s2">+</span><span class="si">%d</span><span class="s2"> = </span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">device_a</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">device_b</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">device_c</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">device_a</span><span class="p">);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">device_b</span><span class="p">);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">device_c</span><span class="p">);</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Writing vector_sum_3.cu
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
source ~/.profile
nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall vector_sum_3.cu -o vector_sum_3.out
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>nvcc warning : The &#39;compute_35&#39;, &#39;compute_37&#39;, &#39;compute_50&#39;, &#39;sm_35&#39;, &#39;sm_37&#39; and &#39;sm_50&#39; architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
source ~/.profile
nvprof --normalized-time-unit s ./vector_sum_3.out
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="arquitectura-de-una-gpu-y-limites-en-numero-de-threads-y-bloques-que-podemos-lanzar-en-el-kernel">
<h2>Arquitectura de una GPU y límites en número de threads y bloques que podemos lanzar en el kernel<a class="headerlink" href="#arquitectura-de-una-gpu-y-limites-en-numero-de-threads-y-bloques-que-podemos-lanzar-en-el-kernel" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="cupy">
<h2><a class="reference external" href="https://github.com/cupy/cupy">CuPy</a><a class="headerlink" href="#cupy" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="gputools">
<h2><a class="reference external" href="https://github.com/nullsatz/gputools">Gputools</a><a class="headerlink" href="#gputools" title="Permalink to this headline">¶</a></h2>
<p>Ver <a class="reference external" href="https://rdrr.io/cran/gputools/">gputools: cran</a></p>
</div>
<div class="section" id="referencias-de-interes">
<h2>Referencias de interés<a class="headerlink" href="#referencias-de-interes" title="Permalink to this headline">¶</a></h2>
<p>Para más sobre <em>Unified Memory</em> revisar:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://devblogs.nvidia.com/even-easier-introduction-cuda/">Even easier introduction to cuda</a></p></li>
<li><p><a class="reference external" href="https://devblogs.nvidia.com/unified-memory-cuda-beginners/">Unified memory cuda beginners</a></p></li>
</ul>
<p>Es importante el manejo de errores por ejemplo en el alojamiento de memoria en la GPU. En este caso es útil revisar:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://devblogs.nvidia.com/how-query-device-properties-and-handle-errors-cuda-cc/">How to Query Device Properties and Handle Errors in CUDA C/C++</a></p></li>
</ul>
<p>En las siguientes preguntas encontramos a personas desarrolladoras de CUDA que las resuelven y resultan muy útiles para continuar con el aprendizaje de CUDA C. Por ejemplo:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://stackoverflow.com/questions/51526082/cuda-parallel-reduction-over-one-axis">Parallel reduction over one axis</a></p></li>
</ul>
<p>Otros sistemas de software para el <a class="reference external" href="https://en.wikipedia.org/wiki/Heterogeneous_computing">Heterogeneous computing</a> son:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/OpenCL">OpenCl</a>. Ver <a class="reference external" href="https://developer.nvidia.com/opencl">NVIDIA OpenCL SDK Code Samples</a> para ejemplos con NVIDIA GPU’s.</p></li>
<li><p><a class="reference external" href="https://github.com/Rth-org/Rth">Rth-org/Rth</a> y más reciente <a class="reference external" href="https://github.com/matloff/Rth">matloff/Rth</a>. Ver también <a class="reference external" href="https://rdrr.io/github/matloff/Rth/f/README.md">rdrr.io matloff/Rth</a>.</p></li>
</ul>
<p>Es posible escribir <a class="reference external" href="https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/II.computo_paralelo/2.3.CUDA_C.ipynb">kernels</a> con CuPy. Ver por ejemplo: <a class="reference external" href="https://docs-cupy.chainer.org/en/stable/tutorial/kernel.html">User-Defined Kernels</a>.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs-cupy.chainer.org/en/stable/index.html">CuPy – NumPy-like API accelerated with CUDA</a></p></li>
<li><p><a class="reference external" href="https://github.com/cupy/cupy">CuPy : NumPy-like API accelerated with CUDA github</a></p></li>
</ul>
<p>Otro paquete para uso de Python+GPU para cómputo matricial es:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/inducer/pycuda/">PyCUDA</a> y ver <a class="reference external" href="https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/tree/master/Python/PyCUDA">PyCUDA en el repo de la clase</a> para más información.</p></li>
</ul>
<p>Un paquete para uso de pandas+GPU:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/rapidsai">Rapids</a>, <a class="reference external" href="https://github.com/rapidsai/cudf">cudf</a></p></li>
</ul>
<p>Ver <a class="reference external" href="https://docs-cupy.chainer.org/en/stable/install.html#optional-libraries">optional-libraries</a> para librerías que pueden ser utilizadas con CuPy.</p>
<div class="tip admonition">
<p class="admonition-title">Ejercicios</p>
<p>1.Resuelve los ejercicios y preguntas de la nota.</p>
</div>
<p><strong>Preguntas de comprehensión:</strong></p>
<p>1)¿Qué factores han determinado un mejor <em>performance</em> de una GPU vs una CPU? (contrasta los diseños de una CPU vs una GPU).</p>
<p>2)¿Dentro de qué modelo de arquitectura de máquinas se ubica a la GPU dentro de la taxonomía de Flynn? (tip: tal modelo se le puede comparar con el modelo <strong>Single Program Multiple Data (SPMD)</strong>)</p>
<p>3)¿Qué significan las siglas CUDA y detalla qué es CUDA?.</p>
<p>4)¿Qué es y en qué consiste CUDA C?</p>
<p>5)¿Qué es un <em>kernel</em>?</p>
<p>6)¿Qué pieza de CUDA se encarga de asignar los bloques de <em>cuda-threads</em> a las SM’s?</p>
<p>7)¿Qué características (recursos compartidos, dimensiones, forma de agendar la ejecución en <em>threads</em>) tienen los bloques que se asignan a una SM al lanzarse y ejecutarse un <em>kernel</em>?</p>
<p>8)¿Qué es un <em>warp</em>?</p>
<p>9)Menciona los tipos de memorias que existen en las GPU’s.</p>
<p>10)Supón que tienes una tarjeta GT200 cuyas características son:</p>
<ul class="simple">
<li><p>Máximo número de <em>threads</em> que soporta una SM en un mismo instante en el tiempo: 1024</p></li>
<li><p>Máximo número de <em>threads</em> en un bloque: 512</p></li>
<li><p>Máximo número de bloques por SM: 8</p></li>
<li><p>Número de SM’s que tiene esta GPU: 30</p></li>
</ul>
<p>Responde:</p>
<p>a)¿Cuál es la máxima cantidad de <em>threads</em> que puede soportar esta GPU en un mismo instante en el tiempo?</p>
<p>b)¿Cuál es la máxima cantidad de <em>warps</em> por SM que puede soportar esta GPU en un mismo instante en el tiempo?</p>
<p>c)¿Cuáles configuraciones de bloques y <em>threads</em> siguientes aprovechan la máxima cantidad de <em>warps</em> en una SM de esta GPU para un mismo instante en el tiempo?</p>
<p>1.Una configuración del tipo: bloques de 64 <em>threads</em> y 16 bloques.</p>
<p>2.Una configuración del tipo: bloques de 1024 <em>threads</em> y 1 bloque.</p>
<p>3.Una configuración del tipo: bloques de 256 <em>threads</em> y 4 bloques.</p>
<p>4.Una configuración del tipo: bloques de 512 <em>threads</em> y 8 bloques.</p>
<p>*Debes considerar las restricciones/características de la GPU dadas para responder pues algunas configuraciones infringen las mismas. No estamos considerando <em>registers</em> o <em>shared memory</em>.</p>
<p><strong>Referencias:</strong></p>
<ol class="simple">
<li><p>N. Matloff, Parallel Computing for Data Science. With Examples in R, C++ and CUDA, 2014.</p></li>
<li><p>D. B. Kirk, W. W. Hwu, Programming Massively Parallel Processors: A Hands-on Approach, Morgan Kaufmann, 2010.</p></li>
<li><p>NVIDIA,CUDA Programming Guide, NVIDIA Corporation, 2007.</p></li>
<li><p>B. W. Kernighan, D. M. Ritchie, The C Programming Language, Prentice Hall Software Series, 1988</p></li>
<li><p><a class="reference external" href="https://github.com/palmoreck/programming-languages/tree/master/C/extensiones_a_C/CUDA">C/extensiones_a_C/CUDA/</a></p></li>
</ol>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "palmoreck/dockerfiles-for-binder",
            ref: "jupyterlab_optimizacion_2",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./V.optimizacion_de_codigo/5.5"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="../5.4/Computo_en_paralelo_usando_CPUS_en_SMC.html" title="previous page">5.4 Cómputo en paralelo usando CPUs en un sistema de memoria compartida (SMC)</a>
    <a class='right-next' id="next-link" href="../../VI.algoritmos_optimizacion_convexa/6.1/Problemas_UCO.html" title="next page">6.1 Problemas tipo <em>Unconstrained Convex Optimization</em> (UCO)</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Erick Palacios Moreno<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>