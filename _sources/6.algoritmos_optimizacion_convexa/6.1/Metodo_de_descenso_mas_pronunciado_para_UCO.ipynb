{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(METDESCMASPRONUN)="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Método de descenso más pronunciado para *Unconstrained Convex Optimization* (UCO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Notas para contenedor de docker:\n",
    "\n",
    "Comando de docker para ejecución de la nota de forma local:\n",
    "\n",
    "nota: cambiar `<ruta a mi directorio>` por la ruta de directorio que se desea mapear a `/datos` dentro del contenedor de docker.\n",
    "\n",
    "`docker run --rm -v <ruta a mi directorio>:/datos --name jupyterlab_optimizacion_2 -p 8888:8888 -p 8787:8787 -d palmoreck/jupyterlab_optimizacion_2:3.0.0`\n",
    "\n",
    "password para jupyterlab: `qwerty`\n",
    "\n",
    "Detener el contenedor de docker:\n",
    "\n",
    "`docker stop jupyterlab_optimizacion_2`\n",
    "\n",
    "Documentación de la imagen de docker `palmoreck/jupyterlab_optimizacion_2:3.0.0` en [liga](https://github.com/palmoreck/dockerfiles/tree/master/jupyterlab/optimizacion_2).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota generada a partir de [liga1](https://drive.google.com/file/d/16-_PvWNaO0Zc9x04-SRsxCRdn5fxebf2/view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Al final de esta nota la comunidad lectora:\n",
    ":class: tip\n",
    "\n",
    "*\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta nota se asume que los problemas de optimización convexa sin restricciones, *Unconstrained Convex Optimization* (UCO) son de la forma:\n",
    "\n",
    "$$\\min f_o(x)$$\n",
    "\n",
    "con $f_o:\\mathbb{R}^n \\rightarrow \\mathbb{R}$ función objetivo fuertemente convexa y $f_o \\in \\mathcal{C}^2(\\text{dom}f_o)$.\n",
    "\n",
    "Además se asume que los puntos iniciales $x^{(0)}$ de los métodos iterativos están en $\\text{dom}f_o$ y los conjuntos $f_o(x^{(0)})$-subnivel son conjuntos cerrados. También se asume que existe un punto óptimo $x^*$ por lo que el problema tiene solución y el valor óptimo se denota por $p^* = f_o(x^*) = \\inf f_o(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentario\n",
    "\n",
    "Las suposición que una función $f$ sea convexa asegura que una **condición necesaria y suficiente** para que $x^*$ sea óptimo es: $\\nabla f(x^*) = 0$ la cual es **en general** es un conjunto de $n$ **ecuaciones no lineales** en $n$ variables y que resuelve el problema de optimización planteado al inicio. Ver {ref}`resultados útiles <RESUT>`. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método de *steepest descent* o descenso más pronunciado sigue los pasos del algoritmo ya revisado en {ref}`método general de descenso para problemas UCO <METGENDESCPARAUCO>` que a continuación nuevamente se describe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo: método general de descenso para problemas UCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "Los pasos de un algoritmo representan una guía para la implementación, no implica que se tengan que implementar uno a continuación del otro como se describe. Si una implementación respeta la lógica y al mismo algoritmo, entonces pueden seguirse los pasos de una forma distinta.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Dado** un **punto inicial** $x$ en $\\text{dom}f_o$\n",
    ">\n",
    "> **Repetir** el siguiente bloque para $k=0,1,2, \\dots$\n",
    ">>\n",
    ">> 1. Determinar una dirección de descenso $\\Delta x$.\n",
    ">> 2. Búsqueda de línea. Elegir un tamaño de paso $t > 0$.\n",
    ">> 3. Hacer la actualización: $x = x + t\\Delta x$.\n",
    ">\n",
    "> **hasta** convergencia (satisfacer criterio de paro).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Perspectiva del método de descenso más pronunciado "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcular una dirección $\\Delta x_{\\text{nsd}}$ que satisfaga: \n",
    "\n",
    "$$\\Delta x_{\\text{nsd}} = \\text{argmin} \\{ \\nabla f_o(x)^Tv : ||v|| \\leq 1, \\nabla f_o(x)^Tv < 0 \\}$$ \n",
    "\n",
    "donde: $||\\cdot||$ es alguna norma en $\\mathbb{R}^n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentario\n",
    "\n",
    "El subíndice \"nsd\" se refiere a *normalized steepest descent*.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente con $\\Delta x_{\\text{nsd}}$ se define el paso $\\Delta x_{\\text{sd}}=||\\nabla f_o(x)||_* \\Delta x_{\\text{nsd}}$ donde: $|| \\cdot||_*$ es la **norma dual**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definición\n",
    "\n",
    "La norma dual asociada a $||\\cdot||$ se denota como $||\\cdot||_*$ y se define como: \n",
    "\n",
    "$$||z||_* = \\sup \\{z^Tx :||x||= 1\\}.$$ \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentarios\n",
    "\n",
    "* Se puede probar que: \n",
    "\n",
    "    * $||z||_{2*}$ (la norma dual de la norma $2$) es $||z||_2$ y $||z||_{1*}$ (la norma dual de la norma $1$) es $||z||_\\infty$ $\\forall z \\in \\mathbb{R}^n$.\n",
    "\n",
    "    * $\\Delta x_{\\text{sd}}$ es dirección de descenso.\n",
    "\n",
    "* En la definición de $\\Delta x_{\\text{nsd}}$ anterior:\n",
    "\n",
    "    * Si $||\\cdot||$ es la norma $2$: $\\Delta x_{\\text{nsd}} = \\text{argmin} \\{ \\nabla f_o(x)^Tv : ||v||_2 \\leq 1, \\nabla f_o(x)^Tv < 0 \\} $ entonces \n",
    "    \n",
    "    $$\\Delta x_{\\text{sd}} = - \\nabla f_o(x).$$\n",
    "    \n",
    "    Lo cual prueba que el método de descenso más pronunciado generaliza al método de descenso en gradiente.\n",
    "\n",
    "    * Si $||\\cdot||$ es una norma cuadrática con matriz $P \\in \\mathbb{S}^n_{++}$: $\\Delta x_{\\text{nsd}} = \\text{argmin} \\{ \\nabla f_o(x)^Tv : ||v||_P \\leq 1, \\nabla f_o(x)^Tv < 0 \\} $ entonces \n",
    "    \n",
    "    $$\\Delta x_{\\text{sd}} = - P^{-1} \\nabla f_o(x).$$\n",
    "    \n",
    "    Si $P=\\nabla ^2 f_o(x^*)$ tenemos el método de Newton por lo que es un caso particular del método de descenso más pronunciado. Si $P=I$ tenemos el método de descenso en gradiente (punto anterior).\n",
    "\n",
    "    * Si $||\\cdot||$ es la norma $1$: $\\Delta x_{\\text{nsd}} = \\text{argmin} \\{ \\nabla f_o(x)^Tv : ||v||_1 \\leq 1, \\nabla f_o(x)^Tv < 0 \\} $ entonces \n",
    "    \n",
    "    $$\\Delta x_{\\text{sd}} = - \\frac{\\partial f_o(x)}{\\partial x_i} e_i$$\n",
    "    \n",
    "con $e_i$ $i$-ésimo vector canónico y el índice $i$ es la entrada del vector $\\nabla f_o(x)$ de máxima magnitud: $i$ tal que $\\left |(\\nabla f_o(x))_i \\right | = ||\\nabla f_o(x)||_\\infty$. En este caso el método se nombra ***coordinate descent***, **descenso por coordenadas**. En cada iteración **una única** componente de $x$ es actualizada.\n",
    "    \n",
    "* **Interpretación:** $\\Delta x_{\\text{nsd}}$ es un paso tal que $||\\Delta x_{\\text{nsd}}|| = 1$ y da el **mayor decrecimiento en la aproximación lineal\\* de $f_o$**. Geométricamente es la dirección en la bola unitaria (generada por $||\\cdot||$) que se **extiende lo más lejos posible en la dirección $-\\nabla f_o(x)$**.\n",
    "\n",
    "\\*Recuérdese que la aproximación lineal a una función $f_o$ está dada por Taylor a primer orden: $f_o(x+v)=\\hat{f}_o(x+v) = f_o(x) + \\nabla f_o(x)^Tv$.\n",
    "\n",
    "* Para visualizar el paso $\\Delta x_{\\text{nsd}}$ se tiene el siguiente dibujo: \n",
    "\n",
    "**Con la norma cuadrática:**\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/91qivndznw2xfgb/steepest_descent_quadratic_norm.png?dl=0\" heigth=\"750\" width=\"750\">\n",
    "\n",
    "**Con la norma 1:**\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/qcbpuhvge9uqgim/steepest_desc_l1_norm.png?dl=0\" heigth=\"750\" width=\"750\">\n",
    "\n",
    "En los dibujos $f = f_o$.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos de los métodos de descenso en gradiente y Newton "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Algoritmo: Método de descenso en gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "Los pasos de un algoritmo representan una guía para la implementación, no implica que se tengan que implementar uno a continuación del otro como se describe. Si una implementación respeta la lógica y al mismo algoritmo, entonces pueden seguirse los pasos de una forma distinta.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Dado** un **punto inicial** $x$ en $\\text{dom}f_o$\n",
    ">\n",
    "> **Repetir** el siguiente bloque para $k=0,1,2,...$\n",
    ">>\n",
    ">> 1. Calcular la dirección de descenso $\\Delta x = - \\nabla f_o(x)$.\n",
    ">> 2. Búsqueda de línea. Elegir un tamaño de paso $t > 0$.\n",
    ">> 3. Hacer la actualización: $x = x + t\\Delta x$.\n",
    ">\n",
    "> **hasta** convergencia (satisfacer criterio de paro).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo: Método de Newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Dado** un **punto inicial** $x$ en $\\text{dom}f_o$\n",
    ">\n",
    "> **Repetir** el siguiente bloque para $k=0,1,2,\\dots$\n",
    ">\n",
    ">> 1. Calcular la dirección de descenso de Newton $\\Delta x_{\\text{nt}} = - \\nabla ^2 f_o(x)^{-1} \\nabla f_o(x)$ y el decremento de Newton al cuadrado: $\\lambda^2(x)=\\nabla f(x)^T \\nabla ^2 f(x)^{-1} \\nabla f(x)$.\n",
    ">> 2. Búsqueda de línea. Elegir un tamaño de paso $t > 0$ (usar el cálculo de $\\lambda (x)$ del paso anterior).\n",
    ">> 3. Hacer la actualización: $x = x + t\\Delta x_{\\text{nt}}$.\n",
    ">\n",
    "> **hasta** convergencia (satisfacer criterio de paro).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos del método de descenso por coordenadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos del método de descenso más pronunciado bajo la norma cuadrática como caso particular del método de descenso en gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible probar que el método de descenso más pronunciado bajo la norma cuadrática $||\\cdot||_P$ es el método de descenso en gradiente aplicado al problema de optimización **después del cambio de coordenadas $\\hat{x}=P^{1/2}x$**. En este cambio de coordenadas $x$ es la variable original por lo que si deseamos utilizar descenso en gradiente utilizamos la inversa de la matriz raíz cuadrada simétrica $P^{1/2}$ quedando la transformación como: $x = P^{-1/2}\\hat{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P^{1/2}$ se nombra **raíz cuadrada simétrica** o ***symmetric squareroot*** y está definida para matrices $P$ simétricas semidefinidas positivas como $P^{1/2}=Qdiag(\\lambda_1^{1/2},\\dots,\\lambda_n^{1/2})Q^T$ con $Q$ y $diag(\\lambda_1^{1/2},\\dots,\\lambda_n^{1/2})$ obtenidas con la **descomposición espectral** de $P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**De la nota descenso en gradiente_Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) $$\\min \\quad (x_1-2)^2+(2-x_2)^2+x_3^2+x_4^4$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) El método de descenso en gradiente es **altamente sensible** a la forma de las curvas de nivel de la función objetivo $f_o$. Para observar esto considérese el problema: $$\\min \\frac{1}{2}\\left(x_1^2+Cx_2^2 \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) $$\\min \\quad e^{(x_1+3x_2-0.1)}+e^{x_1-3x_2-0.1}+e^{-x_1-0.1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "* Como se observa en los ejemplos anteriores la elección de la norma en el método de descenso más pronunciado tiene un efecto fuerte en la tasa de convergencia. \n",
    "\n",
    "* Siempre existe una matriz $P$ para la cual el método de descenso más pronunciado tiene una convergencia buena. El reto está en encontrar tal matriz. La idea es identificar una matriz $P$ para la cual el problema transformado tenga un número de condición moderado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**De la nota descenso por coordenadas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) $$\\min \\quad (x_1-2)^2+(2-x_2)^2+x_3^2+x_4^4$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) $$\\min \\frac{1}{2}\\left(x_1^2+Cx_2^2 \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) $$\\min \\quad e^{(x_1+3x_2-0.1)}+e^{x_1-3x_2-0.1}+e^{-x_1-0.1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**De la nota Método Newton Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) $$\\min \\quad e^{(x_1+3x_2-0.1)}+e^{x_1-3x_2-0.1}+e^{-x_1-0.1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) El método de Newton **es insensible** a la forma de las curvas de nivel de la función objetivo $f_o$. Para observar esto considérese el problema: \n",
    "\n",
    "$$\\min \\frac{1}{2}\\left(x_1^2+Cx_2^2 \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Ejercicios\n",
    ":class: tip\n",
    "\n",
    "1.Resuelve los ejercicios y preguntas de la nota.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas de comprehensión**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referencias:**\n",
    "\n",
    "1. S. P. Boyd, L. Vandenberghe, Convex Optimization, Cambridge University Press, 2009.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
